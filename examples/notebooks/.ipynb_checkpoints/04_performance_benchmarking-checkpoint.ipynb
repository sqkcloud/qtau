{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QTau Performance Benchmarking and Analysis\n",
    "\n",
    "This notebook provides comprehensive tools for benchmarking and analyzing QTau performance across different configurations.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Performance benchmarking is crucial for:\n",
    "- Understanding system capabilities\n",
    "- Identifying bottlenecks\n",
    "- Optimizing resource allocation\n",
    "- Comparing different configurations\n",
    "\n",
    "### Table of Contents\n",
    "1. [Setup and Data Loading](#setup)\n",
    "2. [Throughput Analysis](#throughput)\n",
    "3. [Latency Analysis](#latency)\n",
    "4. [Scaling Efficiency](#scaling)\n",
    "5. [Resource Utilization](#resources)\n",
    "6. [Comparative Analysis](#comparison)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading <a id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from scipy import stats\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "plt.rcParams['figure.figsize'] = [12, 6]\n",
    "plt.rcParams['font.size'] = 11\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"Analysis started: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_benchmark_data(n_runs=100, n_configs=4):\n",
    "    \"\"\"\n",
    "    Generate synthetic benchmark data similar to QTau metrics.csv output.\n",
    "    In production, you would load actual metrics from:\n",
    "    pd.read_csv(os.path.join(pcs.pcs_working_directory, 'metrics.csv'))\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    configs = [\n",
    "        {'nodes': 1, 'cores': 64, 'engine': 'dask'},\n",
    "        {'nodes': 2, 'cores': 128, 'engine': 'dask'},\n",
    "        {'nodes': 2, 'cores': 128, 'engine': 'ray'},\n",
    "        {'nodes': 4, 'cores': 256, 'engine': 'ray'},\n",
    "    ]\n",
    "    \n",
    "    data = []\n",
    "    base_time = datetime.now() - timedelta(hours=2)\n",
    "    \n",
    "    for config_id, config in enumerate(configs):\n",
    "        for run_id in range(n_runs):\n",
    "            # Simulate performance characteristics\n",
    "            base_exec_time = 1.0 / (config['cores'] / 64)  # Faster with more cores\n",
    "            \n",
    "            # Ray has lower latency than Dask\n",
    "            engine_factor = 0.8 if config['engine'] == 'ray' else 1.0\n",
    "            \n",
    "            submit_time = base_time + timedelta(seconds=run_id * 0.1 + config_id * n_runs * 0.1)\n",
    "            wait_time = np.random.exponential(0.1 * engine_factor)\n",
    "            exec_time = np.random.exponential(base_exec_time * engine_factor)\n",
    "            staging_time = np.random.exponential(0.02)\n",
    "            \n",
    "            data.append({\n",
    "                'task_id': f'task-{config_id}-{run_id}',\n",
    "                'config_id': config_id,\n",
    "                'nodes': config['nodes'],\n",
    "                'cores': config['cores'],\n",
    "                'engine': config['engine'],\n",
    "                'qtau_scheduled': f'qtau-{config_id}',\n",
    "                'submit_time': submit_time,\n",
    "                'wait_time_secs': wait_time,\n",
    "                'staging_time_secs': staging_time,\n",
    "                'execution_secs': exec_time,\n",
    "                'completion_time': submit_time + timedelta(seconds=wait_time + exec_time),\n",
    "                'input_staging_data_size_bytes': np.random.randint(1000, 50000),\n",
    "                'status': 'SUCCESS' if np.random.random() > 0.02 else 'FAILED',\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# Generate benchmark data\n",
    "metrics_df = generate_benchmark_data(n_runs=200, n_configs=4)\n",
    "metrics_df['total_time_secs'] = metrics_df['wait_time_secs'] + metrics_df['execution_secs']\n",
    "\n",
    "print(f\"Generated {len(metrics_df)} benchmark records\")\n",
    "metrics_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics by configuration\n",
    "summary = metrics_df.groupby(['config_id', 'engine', 'nodes', 'cores']).agg({\n",
    "    'execution_secs': ['mean', 'std', 'min', 'max', 'count'],\n",
    "    'wait_time_secs': ['mean', 'std'],\n",
    "    'status': lambda x: (x == 'SUCCESS').mean() * 100\n",
    "}).round(4)\n",
    "\n",
    "summary.columns = ['_'.join(col).strip() for col in summary.columns.values]\n",
    "summary = summary.reset_index()\n",
    "summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Throughput Analysis <a id=\"throughput\"></a>\n",
    "\n",
    "Analyze task throughput across different configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate throughput metrics\n",
    "throughput_data = []\n",
    "\n",
    "for config_id in metrics_df['config_id'].unique():\n",
    "    config_data = metrics_df[metrics_df['config_id'] == config_id]\n",
    "    \n",
    "    # Calculate throughput in different ways\n",
    "    total_tasks = len(config_data)\n",
    "    successful_tasks = (config_data['status'] == 'SUCCESS').sum()\n",
    "    total_exec_time = config_data['execution_secs'].sum()\n",
    "    wall_clock_time = (config_data['completion_time'].max() - config_data['submit_time'].min()).total_seconds()\n",
    "    \n",
    "    throughput_data.append({\n",
    "        'config_id': config_id,\n",
    "        'engine': config_data['engine'].iloc[0],\n",
    "        'nodes': config_data['nodes'].iloc[0],\n",
    "        'cores': config_data['cores'].iloc[0],\n",
    "        'total_tasks': total_tasks,\n",
    "        'successful_tasks': successful_tasks,\n",
    "        'wall_clock_time': wall_clock_time,\n",
    "        'total_exec_time': total_exec_time,\n",
    "        'throughput_tasks_per_sec': total_tasks / wall_clock_time,\n",
    "        'avg_exec_time': config_data['execution_secs'].mean(),\n",
    "        'success_rate': successful_tasks / total_tasks * 100\n",
    "    })\n",
    "\n",
    "throughput_df = pd.DataFrame(throughput_data)\n",
    "throughput_df['efficiency'] = throughput_df['throughput_tasks_per_sec'] / throughput_df['cores']\n",
    "throughput_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize throughput analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Throughput comparison\n",
    "ax1 = axes[0, 0]\n",
    "x = np.arange(len(throughput_df))\n",
    "colors = ['steelblue' if e == 'dask' else 'coral' for e in throughput_df['engine']]\n",
    "bars = ax1.bar(x, throughput_df['throughput_tasks_per_sec'], color=colors)\n",
    "ax1.set_xticks(x)\n",
    "ax1.set_xticklabels([f\"{r['engine']}\\n{r['nodes']}N/{r['cores']}C\" \n",
    "                     for _, r in throughput_df.iterrows()], fontsize=9)\n",
    "ax1.set_ylabel('Throughput (tasks/second)')\n",
    "ax1.set_title('Task Throughput by Configuration')\n",
    "\n",
    "# Add value labels\n",
    "for bar, val in zip(bars, throughput_df['throughput_tasks_per_sec']):\n",
    "    ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
    "             f'{val:.1f}', ha='center', va='bottom', fontsize=10)\n",
    "\n",
    "# 2. Parallel efficiency\n",
    "ax2 = axes[0, 1]\n",
    "ax2.bar(x, throughput_df['efficiency'], color=colors, alpha=0.8)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels([f\"{r['engine']}\\n{r['nodes']}N/{r['cores']}C\" \n",
    "                     for _, r in throughput_df.iterrows()], fontsize=9)\n",
    "ax2.set_ylabel('Efficiency (tasks/sec/core)')\n",
    "ax2.set_title('Parallel Efficiency')\n",
    "\n",
    "# 3. Throughput vs cores\n",
    "ax3 = axes[1, 0]\n",
    "for engine in throughput_df['engine'].unique():\n",
    "    engine_data = throughput_df[throughput_df['engine'] == engine]\n",
    "    ax3.plot(engine_data['cores'], engine_data['throughput_tasks_per_sec'], \n",
    "             'o-', label=engine.upper(), linewidth=2, markersize=10)\n",
    "\n",
    "# Ideal scaling reference\n",
    "cores = throughput_df['cores'].unique()\n",
    "base_throughput = throughput_df['throughput_tasks_per_sec'].min()\n",
    "ideal = base_throughput * cores / cores.min()\n",
    "ax3.plot(sorted(cores), sorted(ideal), 'k--', alpha=0.5, label='Ideal Linear')\n",
    "\n",
    "ax3.set_xlabel('Total Cores')\n",
    "ax3.set_ylabel('Throughput (tasks/second)')\n",
    "ax3.set_title('Throughput Scaling')\n",
    "ax3.legend()\n",
    "\n",
    "# 4. Engine comparison radar chart\n",
    "ax4 = axes[1, 1]\n",
    "dask_data = throughput_df[throughput_df['engine'] == 'dask'].mean()\n",
    "ray_data = throughput_df[throughput_df['engine'] == 'ray'].mean()\n",
    "\n",
    "metrics = ['throughput_tasks_per_sec', 'efficiency', 'success_rate']\n",
    "labels = ['Throughput', 'Efficiency', 'Success Rate']\n",
    "\n",
    "# Normalize for comparison\n",
    "max_vals = throughput_df[metrics].max()\n",
    "dask_norm = [dask_data[m] / max_vals[m] for m in metrics]\n",
    "ray_norm = [ray_data[m] / max_vals[m] for m in metrics]\n",
    "\n",
    "x_radar = np.arange(len(labels))\n",
    "width = 0.35\n",
    "ax4.bar(x_radar - width/2, dask_norm, width, label='Dask', color='steelblue')\n",
    "ax4.bar(x_radar + width/2, ray_norm, width, label='Ray', color='coral')\n",
    "ax4.set_xticks(x_radar)\n",
    "ax4.set_xticklabels(labels)\n",
    "ax4.set_ylabel('Normalized Score')\n",
    "ax4.set_title('Engine Comparison (Normalized)')\n",
    "ax4.legend()\n",
    "ax4.set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('throughput_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Latency Analysis <a id=\"latency\"></a>\n",
    "\n",
    "Analyze task latency distributions and identify bottlenecks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate latency percentiles\n",
    "percentiles = [50, 75, 90, 95, 99]\n",
    "latency_analysis = []\n",
    "\n",
    "for config_id in metrics_df['config_id'].unique():\n",
    "    config_data = metrics_df[metrics_df['config_id'] == config_id]\n",
    "    \n",
    "    row = {\n",
    "        'config_id': config_id,\n",
    "        'engine': config_data['engine'].iloc[0],\n",
    "        'cores': config_data['cores'].iloc[0],\n",
    "    }\n",
    "    \n",
    "    for p in percentiles:\n",
    "        row[f'exec_p{p}'] = np.percentile(config_data['execution_secs'], p)\n",
    "        row[f'wait_p{p}'] = np.percentile(config_data['wait_time_secs'], p)\n",
    "        row[f'total_p{p}'] = np.percentile(config_data['total_time_secs'], p)\n",
    "    \n",
    "    latency_analysis.append(row)\n",
    "\n",
    "latency_df = pd.DataFrame(latency_analysis)\n",
    "latency_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize latency analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Execution time distribution by config\n",
    "ax1 = axes[0, 0]\n",
    "config_labels = [f\"{metrics_df[metrics_df['config_id']==i]['engine'].iloc[0]}\\n{metrics_df[metrics_df['config_id']==i]['cores'].iloc[0]}C\" \n",
    "                 for i in metrics_df['config_id'].unique()]\n",
    "data_to_plot = [metrics_df[metrics_df['config_id']==i]['execution_secs'].values \n",
    "                for i in metrics_df['config_id'].unique()]\n",
    "bp = ax1.boxplot(data_to_plot, labels=config_labels, patch_artist=True)\n",
    "colors = ['steelblue', 'steelblue', 'coral', 'coral']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "ax1.set_xlabel('Configuration')\n",
    "ax1.set_ylabel('Execution Time (seconds)')\n",
    "ax1.set_title('Execution Time Distribution')\n",
    "\n",
    "# 2. Wait time vs Execution time\n",
    "ax2 = axes[0, 1]\n",
    "for config_id in metrics_df['config_id'].unique():\n",
    "    config_data = metrics_df[metrics_df['config_id'] == config_id]\n",
    "    color = 'steelblue' if config_data['engine'].iloc[0] == 'dask' else 'coral'\n",
    "    ax2.scatter(config_data['wait_time_secs'], config_data['execution_secs'],\n",
    "                alpha=0.3, c=color, label=f\"{config_data['engine'].iloc[0]}-{config_data['cores'].iloc[0]}C\")\n",
    "ax2.set_xlabel('Wait Time (seconds)')\n",
    "ax2.set_ylabel('Execution Time (seconds)')\n",
    "ax2.set_title('Wait Time vs Execution Time')\n",
    "ax2.legend(loc='upper right')\n",
    "\n",
    "# 3. Latency percentiles\n",
    "ax3 = axes[1, 0]\n",
    "x = np.arange(len(percentiles))\n",
    "width = 0.2\n",
    "for i, (_, row) in enumerate(latency_df.iterrows()):\n",
    "    color = 'steelblue' if row['engine'] == 'dask' else 'coral'\n",
    "    exec_vals = [row[f'exec_p{p}'] for p in percentiles]\n",
    "    ax3.bar(x + i*width, exec_vals, width, label=f\"{row['engine']}-{row['cores']}C\",\n",
    "            color=color, alpha=0.6 + i*0.1)\n",
    "ax3.set_xticks(x + width * 1.5)\n",
    "ax3.set_xticklabels([f'P{p}' for p in percentiles])\n",
    "ax3.set_xlabel('Percentile')\n",
    "ax3.set_ylabel('Execution Time (seconds)')\n",
    "ax3.set_title('Execution Time Percentiles')\n",
    "ax3.legend(fontsize=8)\n",
    "\n",
    "# 4. Latency breakdown (stacked)\n",
    "ax4 = axes[1, 1]\n",
    "config_ids = latency_df['config_id'].values\n",
    "wait_times = [latency_df[latency_df['config_id']==i]['wait_p50'].values[0] for i in config_ids]\n",
    "exec_times = [latency_df[latency_df['config_id']==i]['exec_p50'].values[0] for i in config_ids]\n",
    "\n",
    "ax4.bar(config_labels, wait_times, label='Wait Time (P50)', color='lightblue')\n",
    "ax4.bar(config_labels, exec_times, bottom=wait_times, label='Execution Time (P50)', color='steelblue')\n",
    "ax4.set_xlabel('Configuration')\n",
    "ax4.set_ylabel('Time (seconds)')\n",
    "ax4.set_title('Median Latency Breakdown')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('latency_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nLatency Summary (P50):\")\n",
    "for _, row in latency_df.iterrows():\n",
    "    print(f\"  {row['engine']}-{row['cores']}C: Wait={row['wait_p50']:.3f}s, Exec={row['exec_p50']:.3f}s, Total={row['total_p50']:.3f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Scaling Efficiency <a id=\"scaling\"></a>\n",
    "\n",
    "Analyze strong and weak scaling characteristics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate scaling benchmark data\n",
    "def generate_scaling_data():\n",
    "    \"\"\"Generate data for scaling analysis.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Strong scaling: fixed problem size, varying resources\n",
    "    strong_scaling = []\n",
    "    fixed_tasks = 1024\n",
    "    \n",
    "    for nodes in [1, 2, 4, 8, 16]:\n",
    "        cores = nodes * 64\n",
    "        # Simulate with Amdahl's law (90% parallel, 10% serial)\n",
    "        parallel_fraction = 0.9\n",
    "        serial_time = 10 * (1 - parallel_fraction)\n",
    "        parallel_time = 10 * parallel_fraction / cores * 64\n",
    "        total_time = serial_time + parallel_time + np.random.normal(0, 0.5)\n",
    "        \n",
    "        speedup = 10 / total_time\n",
    "        efficiency = speedup / nodes\n",
    "        \n",
    "        strong_scaling.append({\n",
    "            'nodes': nodes,\n",
    "            'cores': cores,\n",
    "            'tasks': fixed_tasks,\n",
    "            'time': total_time,\n",
    "            'speedup': speedup,\n",
    "            'efficiency': efficiency\n",
    "        })\n",
    "    \n",
    "    # Weak scaling: proportional problem size\n",
    "    weak_scaling = []\n",
    "    tasks_per_core = 16\n",
    "    \n",
    "    for nodes in [1, 2, 4, 8, 16]:\n",
    "        cores = nodes * 64\n",
    "        tasks = cores * tasks_per_core\n",
    "        \n",
    "        # Weak scaling ideally maintains constant time\n",
    "        # Communication overhead grows with nodes\n",
    "        base_time = 5.0\n",
    "        comm_overhead = 0.1 * np.log2(nodes + 1)\n",
    "        total_time = base_time + comm_overhead + np.random.normal(0, 0.2)\n",
    "        \n",
    "        efficiency = base_time / total_time\n",
    "        \n",
    "        weak_scaling.append({\n",
    "            'nodes': nodes,\n",
    "            'cores': cores,\n",
    "            'tasks': tasks,\n",
    "            'time': total_time,\n",
    "            'efficiency': efficiency\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(strong_scaling), pd.DataFrame(weak_scaling)\n",
    "\n",
    "strong_df, weak_df = generate_scaling_data()\n",
    "print(\"Strong Scaling:\")\n",
    "print(strong_df)\n",
    "print(\"\\nWeak Scaling:\")\n",
    "print(weak_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize scaling analysis\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Strong scaling speedup\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(strong_df['nodes'], strong_df['speedup'], 'bo-', linewidth=2, markersize=10, label='Actual')\n",
    "ax1.plot(strong_df['nodes'], strong_df['nodes'], 'r--', linewidth=2, label='Ideal Linear')\n",
    "ax1.set_xlabel('Number of Nodes')\n",
    "ax1.set_ylabel('Speedup')\n",
    "ax1.set_title('Strong Scaling: Speedup')\n",
    "ax1.legend()\n",
    "ax1.set_xscale('log', base=2)\n",
    "ax1.set_yscale('log', base=2)\n",
    "ax1.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "\n",
    "# 2. Strong scaling efficiency\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(strong_df['nodes'], strong_df['efficiency'] * 100, 'go-', linewidth=2, markersize=10)\n",
    "ax2.axhline(100, color='red', linestyle='--', alpha=0.5, label='Ideal (100%)')\n",
    "ax2.axhline(80, color='orange', linestyle='--', alpha=0.5, label='Good (80%)')\n",
    "ax2.set_xlabel('Number of Nodes')\n",
    "ax2.set_ylabel('Parallel Efficiency (%)')\n",
    "ax2.set_title('Strong Scaling: Parallel Efficiency')\n",
    "ax2.legend()\n",
    "ax2.set_ylim(0, 110)\n",
    "\n",
    "# 3. Weak scaling time\n",
    "ax3 = axes[1, 0]\n",
    "ax3.plot(weak_df['nodes'], weak_df['time'], 'mo-', linewidth=2, markersize=10, label='Actual')\n",
    "ax3.axhline(weak_df['time'].iloc[0], color='red', linestyle='--', alpha=0.5, label='Ideal (constant)')\n",
    "ax3.set_xlabel('Number of Nodes')\n",
    "ax3.set_ylabel('Execution Time (seconds)')\n",
    "ax3.set_title('Weak Scaling: Execution Time')\n",
    "ax3.legend()\n",
    "\n",
    "# Add task count annotations\n",
    "for _, row in weak_df.iterrows():\n",
    "    ax3.annotate(f'{int(row[\"tasks\"])} tasks', \n",
    "                 (row['nodes'], row['time']),\n",
    "                 textcoords=\"offset points\", xytext=(0, 10), \n",
    "                 ha='center', fontsize=9)\n",
    "\n",
    "# 4. Weak scaling efficiency\n",
    "ax4 = axes[1, 1]\n",
    "ax4.plot(weak_df['nodes'], weak_df['efficiency'] * 100, 'co-', linewidth=2, markersize=10)\n",
    "ax4.axhline(100, color='red', linestyle='--', alpha=0.5, label='Ideal (100%)')\n",
    "ax4.set_xlabel('Number of Nodes')\n",
    "ax4.set_ylabel('Weak Scaling Efficiency (%)')\n",
    "ax4.set_title('Weak Scaling: Efficiency')\n",
    "ax4.legend()\n",
    "ax4.set_ylim(80, 105)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('scaling_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nScaling Summary:\")\n",
    "print(f\"Strong scaling efficiency at 16 nodes: {strong_df['efficiency'].iloc[-1]*100:.1f}%\")\n",
    "print(f\"Weak scaling efficiency at 16 nodes: {weak_df['efficiency'].iloc[-1]*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Resource Utilization <a id=\"resources\"></a>\n",
    "\n",
    "Analyze CPU, memory, and GPU utilization patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate resource utilization data\n",
    "def generate_resource_data(duration_mins=10, sample_rate_secs=5):\n",
    "    \"\"\"Generate simulated resource utilization data.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    n_samples = int(duration_mins * 60 / sample_rate_secs)\n",
    "    n_workers = 4\n",
    "    \n",
    "    data = []\n",
    "    for t in range(n_samples):\n",
    "        timestamp = datetime.now() - timedelta(minutes=duration_mins) + timedelta(seconds=t * sample_rate_secs)\n",
    "        \n",
    "        for worker_id in range(n_workers):\n",
    "            # Simulate varying utilization with some correlation\n",
    "            base_load = 0.5 + 0.3 * np.sin(t / 20 + worker_id)\n",
    "            cpu_util = np.clip(base_load + np.random.normal(0, 0.1), 0, 1)\n",
    "            memory_util = np.clip(0.3 + 0.5 * cpu_util + np.random.normal(0, 0.05), 0, 1)\n",
    "            gpu_util = np.clip(0.8 * cpu_util + np.random.normal(0, 0.1), 0, 1) if worker_id < 2 else 0\n",
    "            network_mbps = cpu_util * 100 + np.random.exponential(10)\n",
    "            \n",
    "            data.append({\n",
    "                'timestamp': timestamp,\n",
    "                'time_offset': t * sample_rate_secs,\n",
    "                'worker_id': f'worker-{worker_id}',\n",
    "                'cpu_util': cpu_util,\n",
    "                'memory_util': memory_util,\n",
    "                'gpu_util': gpu_util,\n",
    "                'network_mbps': network_mbps\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "resource_df = generate_resource_data()\n",
    "resource_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize resource utilization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. CPU utilization over time\n",
    "ax1 = axes[0, 0]\n",
    "for worker in resource_df['worker_id'].unique():\n",
    "    worker_data = resource_df[resource_df['worker_id'] == worker]\n",
    "    ax1.plot(worker_data['time_offset'], worker_data['cpu_util'] * 100, \n",
    "             alpha=0.7, linewidth=1.5, label=worker)\n",
    "\n",
    "# Add average\n",
    "avg_cpu = resource_df.groupby('time_offset')['cpu_util'].mean() * 100\n",
    "ax1.plot(avg_cpu.index, avg_cpu.values, 'k-', linewidth=2.5, label='Average')\n",
    "\n",
    "ax1.set_xlabel('Time (seconds)')\n",
    "ax1.set_ylabel('CPU Utilization (%)')\n",
    "ax1.set_title('CPU Utilization Over Time')\n",
    "ax1.legend(loc='upper right', fontsize=8)\n",
    "ax1.set_ylim(0, 100)\n",
    "\n",
    "# 2. Resource utilization heatmap\n",
    "ax2 = axes[0, 1]\n",
    "pivot_cpu = resource_df.pivot_table(index='worker_id', columns='time_offset', \n",
    "                                      values='cpu_util', aggfunc='mean')\n",
    "# Sample every 10th column for clarity\n",
    "pivot_cpu_sampled = pivot_cpu.iloc[:, ::10]\n",
    "sns.heatmap(pivot_cpu_sampled, cmap='YlOrRd', ax=ax2, \n",
    "            cbar_kws={'label': 'CPU Utilization'})\n",
    "ax2.set_xlabel('Time (sampled)')\n",
    "ax2.set_ylabel('Worker')\n",
    "ax2.set_title('CPU Utilization Heatmap')\n",
    "\n",
    "# 3. Resource distribution box plots\n",
    "ax3 = axes[1, 0]\n",
    "resource_cols = ['cpu_util', 'memory_util', 'gpu_util']\n",
    "resource_labels = ['CPU', 'Memory', 'GPU']\n",
    "data_to_plot = [resource_df[col].values * 100 for col in resource_cols]\n",
    "bp = ax3.boxplot(data_to_plot, labels=resource_labels, patch_artist=True)\n",
    "colors = ['steelblue', 'coral', 'green']\n",
    "for patch, color in zip(bp['boxes'], colors):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.7)\n",
    "ax3.set_ylabel('Utilization (%)')\n",
    "ax3.set_title('Resource Utilization Distribution')\n",
    "\n",
    "# 4. Average utilization by worker\n",
    "ax4 = axes[1, 1]\n",
    "avg_by_worker = resource_df.groupby('worker_id')[['cpu_util', 'memory_util', 'gpu_util']].mean() * 100\n",
    "avg_by_worker.plot(kind='bar', ax=ax4, width=0.8)\n",
    "ax4.set_xlabel('Worker')\n",
    "ax4.set_ylabel('Average Utilization (%)')\n",
    "ax4.set_title('Average Resource Utilization by Worker')\n",
    "ax4.legend(['CPU', 'Memory', 'GPU'])\n",
    "ax4.tick_params(axis='x', rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('resource_utilization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nResource Utilization Summary:\")\n",
    "print(f\"  Avg CPU: {resource_df['cpu_util'].mean()*100:.1f}%\")\n",
    "print(f\"  Avg Memory: {resource_df['memory_util'].mean()*100:.1f}%\")\n",
    "print(f\"  Avg GPU: {resource_df[resource_df['gpu_util']>0]['gpu_util'].mean()*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Comparative Analysis <a id=\"comparison\"></a>\n",
    "\n",
    "Compare different execution engines and configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistical comparison between Dask and Ray\n",
    "dask_data = metrics_df[metrics_df['engine'] == 'dask']['execution_secs']\n",
    "ray_data = metrics_df[metrics_df['engine'] == 'ray']['execution_secs']\n",
    "\n",
    "# Perform statistical tests\n",
    "t_stat, t_pvalue = stats.ttest_ind(dask_data, ray_data)\n",
    "u_stat, u_pvalue = stats.mannwhitneyu(dask_data, ray_data)\n",
    "\n",
    "comparison_stats = {\n",
    "    'Metric': ['Mean Execution Time', 'Median Execution Time', 'Std Deviation', \n",
    "               'Min', 'Max', '99th Percentile'],\n",
    "    'Dask': [\n",
    "        f\"{dask_data.mean():.4f}s\",\n",
    "        f\"{dask_data.median():.4f}s\",\n",
    "        f\"{dask_data.std():.4f}s\",\n",
    "        f\"{dask_data.min():.4f}s\",\n",
    "        f\"{dask_data.max():.4f}s\",\n",
    "        f\"{np.percentile(dask_data, 99):.4f}s\"\n",
    "    ],\n",
    "    'Ray': [\n",
    "        f\"{ray_data.mean():.4f}s\",\n",
    "        f\"{ray_data.median():.4f}s\",\n",
    "        f\"{ray_data.std():.4f}s\",\n",
    "        f\"{ray_data.min():.4f}s\",\n",
    "        f\"{ray_data.max():.4f}s\",\n",
    "        f\"{np.percentile(ray_data, 99):.4f}s\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "stats_df = pd.DataFrame(comparison_stats)\n",
    "print(\"Statistical Comparison:\")\n",
    "print(stats_df.to_string(index=False))\n",
    "print(f\"\\nT-test p-value: {t_pvalue:.4e}\")\n",
    "print(f\"Mann-Whitney U p-value: {u_pvalue:.4e}\")\n",
    "print(f\"\\nConclusion: {'Significant difference' if t_pvalue < 0.05 else 'No significant difference'} at α=0.05\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final comprehensive comparison visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# 1. Execution time distributions\n",
    "ax1 = axes[0, 0]\n",
    "ax1.hist(dask_data, bins=30, alpha=0.6, label='Dask', color='steelblue', density=True)\n",
    "ax1.hist(ray_data, bins=30, alpha=0.6, label='Ray', color='coral', density=True)\n",
    "ax1.axvline(dask_data.mean(), color='steelblue', linestyle='--', linewidth=2)\n",
    "ax1.axvline(ray_data.mean(), color='coral', linestyle='--', linewidth=2)\n",
    "ax1.set_xlabel('Execution Time (seconds)')\n",
    "ax1.set_ylabel('Density')\n",
    "ax1.set_title('Execution Time Distribution')\n",
    "ax1.legend()\n",
    "\n",
    "# 2. CDF comparison\n",
    "ax2 = axes[0, 1]\n",
    "dask_sorted = np.sort(dask_data)\n",
    "ray_sorted = np.sort(ray_data)\n",
    "ax2.plot(dask_sorted, np.linspace(0, 1, len(dask_sorted)), \n",
    "         'b-', linewidth=2, label='Dask')\n",
    "ax2.plot(ray_sorted, np.linspace(0, 1, len(ray_sorted)), \n",
    "         'r-', linewidth=2, label='Ray')\n",
    "ax2.set_xlabel('Execution Time (seconds)')\n",
    "ax2.set_ylabel('Cumulative Probability')\n",
    "ax2.set_title('Cumulative Distribution Function')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "# 3. Violin plot\n",
    "ax3 = axes[0, 2]\n",
    "parts = ax3.violinplot([dask_data, ray_data], positions=[1, 2], showmeans=True, showmedians=True)\n",
    "parts['bodies'][0].set_facecolor('steelblue')\n",
    "parts['bodies'][1].set_facecolor('coral')\n",
    "ax3.set_xticks([1, 2])\n",
    "ax3.set_xticklabels(['Dask', 'Ray'])\n",
    "ax3.set_ylabel('Execution Time (seconds)')\n",
    "ax3.set_title('Execution Time Violin Plot')\n",
    "\n",
    "# 4. Throughput comparison\n",
    "ax4 = axes[1, 0]\n",
    "throughput_comparison = throughput_df.groupby('engine').agg({\n",
    "    'throughput_tasks_per_sec': 'mean',\n",
    "    'efficiency': 'mean',\n",
    "    'success_rate': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "x = np.arange(len(throughput_comparison))\n",
    "ax4.bar(x, throughput_comparison['throughput_tasks_per_sec'], \n",
    "        color=['steelblue', 'coral'])\n",
    "ax4.set_xticks(x)\n",
    "ax4.set_xticklabels(throughput_comparison['engine'].str.upper())\n",
    "ax4.set_ylabel('Avg Throughput (tasks/sec)')\n",
    "ax4.set_title('Average Throughput by Engine')\n",
    "\n",
    "# 5. Efficiency comparison\n",
    "ax5 = axes[1, 1]\n",
    "ax5.bar(x, throughput_comparison['efficiency'] * 1000, color=['steelblue', 'coral'])\n",
    "ax5.set_xticks(x)\n",
    "ax5.set_xticklabels(throughput_comparison['engine'].str.upper())\n",
    "ax5.set_ylabel('Avg Efficiency (×10⁻³ tasks/sec/core)')\n",
    "ax5.set_title('Average Efficiency by Engine')\n",
    "\n",
    "# 6. Summary radar chart\n",
    "ax6 = axes[1, 2]\n",
    "categories = ['Throughput', 'Efficiency', 'Success Rate', 'Low Latency']\n",
    "\n",
    "# Normalize metrics for radar chart\n",
    "dask_metrics = [\n",
    "    throughput_comparison[throughput_comparison['engine']=='dask']['throughput_tasks_per_sec'].values[0] / throughput_df['throughput_tasks_per_sec'].max(),\n",
    "    throughput_comparison[throughput_comparison['engine']=='dask']['efficiency'].values[0] / throughput_df['efficiency'].max(),\n",
    "    throughput_comparison[throughput_comparison['engine']=='dask']['success_rate'].values[0] / 100,\n",
    "    1 - dask_data.mean() / metrics_df['execution_secs'].max()\n",
    "]\n",
    "\n",
    "ray_metrics = [\n",
    "    throughput_comparison[throughput_comparison['engine']=='ray']['throughput_tasks_per_sec'].values[0] / throughput_df['throughput_tasks_per_sec'].max(),\n",
    "    throughput_comparison[throughput_comparison['engine']=='ray']['efficiency'].values[0] / throughput_df['efficiency'].max(),\n",
    "    throughput_comparison[throughput_comparison['engine']=='ray']['success_rate'].values[0] / 100,\n",
    "    1 - ray_data.mean() / metrics_df['execution_secs'].max()\n",
    "]\n",
    "\n",
    "x_radar = np.arange(len(categories))\n",
    "width = 0.35\n",
    "ax6.bar(x_radar - width/2, dask_metrics, width, label='Dask', color='steelblue', alpha=0.8)\n",
    "ax6.bar(x_radar + width/2, ray_metrics, width, label='Ray', color='coral', alpha=0.8)\n",
    "ax6.set_xticks(x_radar)\n",
    "ax6.set_xticklabels(categories, fontsize=9)\n",
    "ax6.set_ylabel('Normalized Score')\n",
    "ax6.set_title('Overall Performance Comparison')\n",
    "ax6.legend()\n",
    "ax6.set_ylim(0, 1.1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('comparative_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"BENCHMARK SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"\\nTotal benchmarks run: {len(metrics_df)}\")\n",
    "print(f\"Configurations tested: {metrics_df['config_id'].nunique()}\")\n",
    "print(f\"\\nBest performing configuration:\")\n",
    "best_config = throughput_df.loc[throughput_df['throughput_tasks_per_sec'].idxmax()]\n",
    "print(f\"  Engine: {best_config['engine'].upper()}\")\n",
    "print(f\"  Nodes: {best_config['nodes']}\")\n",
    "print(f\"  Cores: {best_config['cores']}\")\n",
    "print(f\"  Throughput: {best_config['throughput_tasks_per_sec']:.2f} tasks/sec\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook provided comprehensive performance benchmarking and analysis for QTau:\n",
    "\n",
    "1. **Throughput Analysis**: Task completion rates across configurations\n",
    "2. **Latency Analysis**: Execution time distributions and percentiles\n",
    "3. **Scaling Efficiency**: Strong and weak scaling characteristics\n",
    "4. **Resource Utilization**: CPU, memory, and GPU usage patterns\n",
    "5. **Comparative Analysis**: Statistical comparison between execution engines\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Performance varies significantly between Dask and Ray\n",
    "- Scaling efficiency decreases with more nodes due to communication overhead\n",
    "- Resource utilization should be monitored to identify bottlenecks\n",
    "- Statistical tests help validate performance differences"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
