{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QTAU with Dask: Distributed Computing Examples\n",
    "\n",
    "This notebook demonstrates how to use QTAU with the **Dask** execution engine for distributed computing tasks.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Dask is a flexible parallel computing library that integrates seamlessly with QTAU. It's ideal for:\n",
    "- Data-parallel computations\n",
    "- Task scheduling with complex dependencies\n",
    "- Integration with pandas, NumPy, and scikit-learn workflows\n",
    "\n",
    "### Table of Contents\n",
    "1. [Setup and Configuration](#setup)\n",
    "2. [Basic Dask Task Distribution](#basic)\n",
    "3. [Parallel Data Processing](#data-processing)\n",
    "4. [Distributed NumPy Operations](#numpy)\n",
    "5. [Task Dependencies and DAGs](#dag)\n",
    "6. [Monitoring and Visualization](#monitoring)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration <a id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# QTAU imports\n",
    "from qtau.pilot_compute_service import PilotComputeService, ExecutionEngine\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"Current time: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Dask execution\n",
    "RESOURCE_URL = \"ssh://localhost\"\n",
    "WORKING_DIRECTORY = os.path.join(os.environ[\"HOME\"], \"work\", \"qtau_dask\")\n",
    "\n",
    "# Create working directory\n",
    "os.makedirs(WORKING_DIRECTORY, exist_ok=True)\n",
    "\n",
    "# Dask pilot configuration\n",
    "pilot_compute_description_dask = {\n",
    "    \"resource\": RESOURCE_URL,\n",
    "    \"working_directory\": WORKING_DIRECTORY,\n",
    "    \"type\": \"dask\",\n",
    "    \"number_of_nodes\": 1,\n",
    "    \"cores_per_node\": 4,\n",
    "}\n",
    "\n",
    "print(\"Dask Configuration:\")\n",
    "for key, value in pilot_compute_description_dask.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting QTAU with Dask (Production Code)\n",
    "\n",
    "```python\n",
    "# Initialize PilotComputeService with Dask engine\n",
    "pcs = PilotComputeService(working_directory=WORKING_DIRECTORY)\n",
    "\n",
    "# Create and wait for pilot\n",
    "pilot = pcs.create_pilot(pilot_compute_description=pilot_compute_description_dask)\n",
    "pilot.wait()\n",
    "\n",
    "# Get Dask client for advanced operations\n",
    "dask_client = pilot.get_client()\n",
    "print(dask_client.scheduler_info())\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Dask Task Distribution <a id=\"basic\"></a>\n",
    "\n",
    "This section demonstrates basic task submission and result collection with Dask."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sample computational tasks\n",
    "def compute_prime_count(n):\n",
    "    \"\"\"Count prime numbers up to n using Sieve of Eratosthenes.\"\"\"\n",
    "    if n < 2:\n",
    "        return 0\n",
    "    sieve = [True] * (n + 1)\n",
    "    sieve[0] = sieve[1] = False\n",
    "    for i in range(2, int(n**0.5) + 1):\n",
    "        if sieve[i]:\n",
    "            for j in range(i*i, n + 1, i):\n",
    "                sieve[j] = False\n",
    "    return sum(sieve)\n",
    "\n",
    "def matrix_operation(size, operation='multiply'):\n",
    "    \"\"\"Perform matrix operations.\"\"\"\n",
    "    A = np.random.rand(size, size)\n",
    "    B = np.random.rand(size, size)\n",
    "    if operation == 'multiply':\n",
    "        result = np.dot(A, B)\n",
    "    elif operation == 'eigenvalue':\n",
    "        result = np.linalg.eigvals(A)\n",
    "    elif operation == 'svd':\n",
    "        result = np.linalg.svd(A, compute_uv=False)\n",
    "    return result.sum() if hasattr(result, 'sum') else sum(result)\n",
    "\n",
    "def simulate_data_processing(chunk_size):\n",
    "    \"\"\"Simulate data processing on a chunk.\"\"\"\n",
    "    data = np.random.randn(chunk_size)\n",
    "    # Simulate various statistics\n",
    "    return {\n",
    "        'mean': np.mean(data),\n",
    "        'std': np.std(data),\n",
    "        'min': np.min(data),\n",
    "        'max': np.max(data),\n",
    "        'sum': np.sum(data)\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate distributed task execution\n",
    "np.random.seed(42)\n",
    "\n",
    "# Prime counting tasks\n",
    "n_values = [10000, 50000, 100000, 200000, 500000]\n",
    "prime_results = []\n",
    "\n",
    "for n in n_values:\n",
    "    start = time.time()\n",
    "    count = compute_prime_count(n)\n",
    "    elapsed = time.time() - start\n",
    "    prime_results.append({\n",
    "        'n': n,\n",
    "        'prime_count': count,\n",
    "        'execution_time': elapsed,\n",
    "        'primes_per_second': count / elapsed if elapsed > 0 else 0\n",
    "    })\n",
    "\n",
    "prime_df = pd.DataFrame(prime_results)\n",
    "prime_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize prime counting results\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 1. Prime count vs n\n",
    "ax1 = axes[0]\n",
    "ax1.plot(prime_df['n'], prime_df['prime_count'], 'bo-', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Upper Bound (n)')\n",
    "ax1.set_ylabel('Prime Count')\n",
    "ax1.set_title('Prime Counting Function π(n)')\n",
    "ax1.ticklabel_format(style='scientific', axis='x', scilimits=(0,0))\n",
    "\n",
    "# Add theoretical approximation n/ln(n)\n",
    "n_theory = np.linspace(10000, 500000, 100)\n",
    "pi_approx = n_theory / np.log(n_theory)\n",
    "ax1.plot(n_theory, pi_approx, 'r--', alpha=0.7, label='n/ln(n) approximation')\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Execution time vs n\n",
    "ax2 = axes[1]\n",
    "ax2.bar(range(len(prime_df)), prime_df['execution_time'], color='steelblue')\n",
    "ax2.set_xticks(range(len(prime_df)))\n",
    "ax2.set_xticklabels([f'{n/1000:.0f}K' for n in prime_df['n']])\n",
    "ax2.set_xlabel('Upper Bound (n)')\n",
    "ax2.set_ylabel('Execution Time (seconds)')\n",
    "ax2.set_title('Computation Time')\n",
    "\n",
    "# 3. Throughput\n",
    "ax3 = axes[2]\n",
    "ax3.plot(prime_df['n'], prime_df['primes_per_second'], 'go-', linewidth=2, markersize=8)\n",
    "ax3.set_xlabel('Upper Bound (n)')\n",
    "ax3.set_ylabel('Primes Found per Second')\n",
    "ax3.set_title('Throughput Analysis')\n",
    "ax3.ticklabel_format(style='scientific', axis='both', scilimits=(0,0))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dask_prime_counting.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Parallel Data Processing <a id=\"data-processing\"></a>\n",
    "\n",
    "Demonstrates how to process data in parallel chunks using Dask with QTAU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate parallel data processing\n",
    "n_chunks = 16\n",
    "chunk_size = 100000\n",
    "\n",
    "# Process chunks (simulating distributed execution)\n",
    "chunk_results = []\n",
    "np.random.seed(42)\n",
    "\n",
    "for i in range(n_chunks):\n",
    "    start = time.time()\n",
    "    result = simulate_data_processing(chunk_size)\n",
    "    elapsed = time.time() - start\n",
    "    result['chunk_id'] = i\n",
    "    result['processing_time'] = elapsed\n",
    "    chunk_results.append(result)\n",
    "\n",
    "chunks_df = pd.DataFrame(chunk_results)\n",
    "chunks_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregate results (like MapReduce)\n",
    "total_sum = chunks_df['sum'].sum()\n",
    "global_mean = chunks_df['mean'].mean()\n",
    "global_std = np.sqrt((chunks_df['std']**2).mean())  # Approximate combined std\n",
    "global_min = chunks_df['min'].min()\n",
    "global_max = chunks_df['max'].max()\n",
    "\n",
    "print(\"Aggregated Statistics Across All Chunks:\")\n",
    "print(f\"  Total Sum: {total_sum:,.2f}\")\n",
    "print(f\"  Global Mean: {global_mean:.6f}\")\n",
    "print(f\"  Global Std: {global_std:.6f}\")\n",
    "print(f\"  Global Min: {global_min:.6f}\")\n",
    "print(f\"  Global Max: {global_max:.6f}\")\n",
    "print(f\"  Total Data Points: {n_chunks * chunk_size:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize chunk processing results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 1. Mean by chunk\n",
    "ax1 = axes[0, 0]\n",
    "ax1.bar(chunks_df['chunk_id'], chunks_df['mean'], color='steelblue', alpha=0.7)\n",
    "ax1.axhline(global_mean, color='red', linestyle='--', label=f'Global Mean: {global_mean:.4f}')\n",
    "ax1.set_xlabel('Chunk ID')\n",
    "ax1.set_ylabel('Mean Value')\n",
    "ax1.set_title('Mean Value by Chunk')\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Standard deviation by chunk\n",
    "ax2 = axes[0, 1]\n",
    "ax2.bar(chunks_df['chunk_id'], chunks_df['std'], color='coral', alpha=0.7)\n",
    "ax2.axhline(global_std, color='red', linestyle='--', label=f'Pooled Std: {global_std:.4f}')\n",
    "ax2.set_xlabel('Chunk ID')\n",
    "ax2.set_ylabel('Standard Deviation')\n",
    "ax2.set_title('Standard Deviation by Chunk')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Min/Max range by chunk\n",
    "ax3 = axes[1, 0]\n",
    "chunk_ids = chunks_df['chunk_id']\n",
    "ax3.fill_between(chunk_ids, chunks_df['min'], chunks_df['max'], alpha=0.3, color='green')\n",
    "ax3.plot(chunk_ids, chunks_df['min'], 'g-', label='Min')\n",
    "ax3.plot(chunk_ids, chunks_df['max'], 'g-', label='Max')\n",
    "ax3.plot(chunk_ids, chunks_df['mean'], 'b-', linewidth=2, label='Mean')\n",
    "ax3.set_xlabel('Chunk ID')\n",
    "ax3.set_ylabel('Value')\n",
    "ax3.set_title('Data Range by Chunk')\n",
    "ax3.legend()\n",
    "\n",
    "# 4. Processing time distribution\n",
    "ax4 = axes[1, 1]\n",
    "ax4.hist(chunks_df['processing_time'] * 1000, bins=10, color='purple', alpha=0.7, edgecolor='black')\n",
    "ax4.axvline(chunks_df['processing_time'].mean() * 1000, color='red', linestyle='--', \n",
    "            label=f'Mean: {chunks_df[\"processing_time\"].mean()*1000:.2f}ms')\n",
    "ax4.set_xlabel('Processing Time (ms)')\n",
    "ax4.set_ylabel('Frequency')\n",
    "ax4.set_title('Processing Time Distribution')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dask_parallel_processing.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Distributed NumPy Operations <a id=\"numpy\"></a>\n",
    "\n",
    "Demonstrates distributed matrix operations commonly used in scientific computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix operation benchmarks\n",
    "matrix_sizes = [100, 200, 300, 400, 500]\n",
    "operations = ['multiply', 'eigenvalue', 'svd']\n",
    "\n",
    "matrix_results = []\n",
    "\n",
    "for size in matrix_sizes:\n",
    "    for op in operations:\n",
    "        start = time.time()\n",
    "        result = matrix_operation(size, op)\n",
    "        elapsed = time.time() - start\n",
    "        matrix_results.append({\n",
    "            'size': size,\n",
    "            'operation': op,\n",
    "            'execution_time': elapsed,\n",
    "            'flops_estimate': size**3 / elapsed if elapsed > 0 else 0\n",
    "        })\n",
    "\n",
    "matrix_df = pd.DataFrame(matrix_results)\n",
    "matrix_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize matrix operation benchmarks\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "# 1. Execution time by operation and size\n",
    "ax1 = axes[0]\n",
    "pivot_time = matrix_df.pivot(index='size', columns='operation', values='execution_time')\n",
    "pivot_time.plot(kind='bar', ax=ax1, width=0.8)\n",
    "ax1.set_xlabel('Matrix Size')\n",
    "ax1.set_ylabel('Execution Time (seconds)')\n",
    "ax1.set_title('Execution Time by Matrix Size and Operation')\n",
    "ax1.tick_params(axis='x', rotation=0)\n",
    "ax1.legend(title='Operation')\n",
    "\n",
    "# 2. Scaling analysis (log-log plot)\n",
    "ax2 = axes[1]\n",
    "for op in operations:\n",
    "    op_data = matrix_df[matrix_df['operation'] == op]\n",
    "    ax2.loglog(op_data['size'], op_data['execution_time'], 'o-', label=op, linewidth=2, markersize=8)\n",
    "\n",
    "# Add O(n^3) reference line\n",
    "sizes = np.array(matrix_sizes)\n",
    "ref_line = (sizes/100)**3 * matrix_df[matrix_df['size']==100]['execution_time'].mean()\n",
    "ax2.loglog(sizes, ref_line, 'k--', alpha=0.5, label='O(n³) reference')\n",
    "\n",
    "ax2.set_xlabel('Matrix Size')\n",
    "ax2.set_ylabel('Execution Time (seconds)')\n",
    "ax2.set_title('Scaling Analysis (Log-Log)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "\n",
    "# 3. Throughput comparison\n",
    "ax3 = axes[2]\n",
    "pivot_flops = matrix_df.pivot(index='size', columns='operation', values='flops_estimate')\n",
    "pivot_flops.plot(kind='line', ax=ax3, marker='o', linewidth=2, markersize=8)\n",
    "ax3.set_xlabel('Matrix Size')\n",
    "ax3.set_ylabel('Estimated FLOPS')\n",
    "ax3.set_title('Computational Throughput')\n",
    "ax3.ticklabel_format(style='scientific', axis='y', scilimits=(0,0))\n",
    "ax3.legend(title='Operation')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dask_matrix_operations.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Task Dependencies and DAGs <a id=\"dag\"></a>\n",
    "\n",
    "Dask excels at handling complex task dependencies. This section visualizes task execution DAGs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a DAG of dependent tasks\n",
    "# Task structure:\n",
    "# Level 0: Data loading (4 tasks)\n",
    "# Level 1: Preprocessing (4 tasks, each depends on 1 load task)\n",
    "# Level 2: Feature extraction (2 tasks, each depends on 2 preprocess tasks)\n",
    "# Level 3: Model training (1 task, depends on all feature tasks)\n",
    "# Level 4: Evaluation (1 task, depends on model training)\n",
    "\n",
    "dag_tasks = [\n",
    "    # Level 0: Data loading\n",
    "    {'task_id': 'load_0', 'level': 0, 'dependencies': [], 'duration': 0.5},\n",
    "    {'task_id': 'load_1', 'level': 0, 'dependencies': [], 'duration': 0.6},\n",
    "    {'task_id': 'load_2', 'level': 0, 'dependencies': [], 'duration': 0.4},\n",
    "    {'task_id': 'load_3', 'level': 0, 'dependencies': [], 'duration': 0.55},\n",
    "    # Level 1: Preprocessing\n",
    "    {'task_id': 'preprocess_0', 'level': 1, 'dependencies': ['load_0'], 'duration': 0.8},\n",
    "    {'task_id': 'preprocess_1', 'level': 1, 'dependencies': ['load_1'], 'duration': 0.7},\n",
    "    {'task_id': 'preprocess_2', 'level': 1, 'dependencies': ['load_2'], 'duration': 0.9},\n",
    "    {'task_id': 'preprocess_3', 'level': 1, 'dependencies': ['load_3'], 'duration': 0.75},\n",
    "    # Level 2: Feature extraction\n",
    "    {'task_id': 'features_0', 'level': 2, 'dependencies': ['preprocess_0', 'preprocess_1'], 'duration': 1.2},\n",
    "    {'task_id': 'features_1', 'level': 2, 'dependencies': ['preprocess_2', 'preprocess_3'], 'duration': 1.1},\n",
    "    # Level 3: Model training\n",
    "    {'task_id': 'train', 'level': 3, 'dependencies': ['features_0', 'features_1'], 'duration': 2.0},\n",
    "    # Level 4: Evaluation\n",
    "    {'task_id': 'evaluate', 'level': 4, 'dependencies': ['train'], 'duration': 0.5},\n",
    "]\n",
    "\n",
    "dag_df = pd.DataFrame(dag_tasks)\n",
    "dag_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate start and end times based on dependencies\n",
    "task_times = {}\n",
    "\n",
    "for _, task in dag_df.iterrows():\n",
    "    task_id = task['task_id']\n",
    "    deps = task['dependencies']\n",
    "    \n",
    "    if not deps:\n",
    "        start_time = 0\n",
    "    else:\n",
    "        # Start after all dependencies complete\n",
    "        start_time = max(task_times[dep]['end'] for dep in deps)\n",
    "    \n",
    "    end_time = start_time + task['duration']\n",
    "    task_times[task_id] = {\n",
    "        'start': start_time,\n",
    "        'end': end_time,\n",
    "        'duration': task['duration'],\n",
    "        'level': task['level']\n",
    "    }\n",
    "\n",
    "# Add to dataframe\n",
    "dag_df['start_time'] = dag_df['task_id'].map(lambda x: task_times[x]['start'])\n",
    "dag_df['end_time'] = dag_df['task_id'].map(lambda x: task_times[x]['end'])\n",
    "\n",
    "print(\"Task Schedule:\")\n",
    "print(dag_df[['task_id', 'level', 'start_time', 'end_time', 'duration']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize DAG execution\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# 1. Gantt chart of task execution\n",
    "ax1 = axes[0]\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, 5))\n",
    "level_colors = {0: colors[0], 1: colors[1], 2: colors[2], 3: colors[3], 4: colors[4]}\n",
    "\n",
    "for i, (_, task) in enumerate(dag_df.iterrows()):\n",
    "    ax1.barh(i, task['duration'], left=task['start_time'], \n",
    "             color=level_colors[task['level']], edgecolor='black', linewidth=0.5)\n",
    "    ax1.text(task['start_time'] + task['duration']/2, i, task['task_id'], \n",
    "             ha='center', va='center', fontsize=8, fontweight='bold')\n",
    "\n",
    "ax1.set_yticks(range(len(dag_df)))\n",
    "ax1.set_yticklabels(dag_df['task_id'])\n",
    "ax1.set_xlabel('Time (seconds)')\n",
    "ax1.set_ylabel('Task')\n",
    "ax1.set_title('DAG Task Execution Timeline')\n",
    "\n",
    "# Add legend for levels\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=level_colors[i], label=f'Level {i}') for i in range(5)]\n",
    "ax1.legend(handles=legend_elements, loc='lower right')\n",
    "\n",
    "# 2. DAG structure visualization\n",
    "ax2 = axes[1]\n",
    "\n",
    "# Position tasks by level\n",
    "level_counts = dag_df.groupby('level').size()\n",
    "positions = {}\n",
    "for level in range(5):\n",
    "    level_tasks = dag_df[dag_df['level'] == level]['task_id'].tolist()\n",
    "    n_tasks = len(level_tasks)\n",
    "    for i, task_id in enumerate(level_tasks):\n",
    "        x = level\n",
    "        y = (i - (n_tasks - 1) / 2) * 1.5\n",
    "        positions[task_id] = (x, y)\n",
    "\n",
    "# Draw edges (dependencies)\n",
    "for _, task in dag_df.iterrows():\n",
    "    task_id = task['task_id']\n",
    "    for dep in task['dependencies']:\n",
    "        x1, y1 = positions[dep]\n",
    "        x2, y2 = positions[task_id]\n",
    "        ax2.annotate('', xy=(x2-0.1, y2), xytext=(x1+0.1, y1),\n",
    "                     arrowprops=dict(arrowstyle='->', color='gray', alpha=0.6))\n",
    "\n",
    "# Draw nodes\n",
    "for task_id, (x, y) in positions.items():\n",
    "    level = dag_df[dag_df['task_id'] == task_id]['level'].values[0]\n",
    "    circle = plt.Circle((x, y), 0.25, color=level_colors[level], ec='black', linewidth=2)\n",
    "    ax2.add_patch(circle)\n",
    "    ax2.text(x, y, task_id.split('_')[0][:4], ha='center', va='center', fontsize=7, fontweight='bold')\n",
    "\n",
    "ax2.set_xlim(-0.5, 4.5)\n",
    "ax2.set_ylim(-3, 3)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.set_xlabel('Pipeline Stage')\n",
    "ax2.set_title('DAG Structure')\n",
    "ax2.set_xticks(range(5))\n",
    "ax2.set_xticklabels(['Load', 'Preprocess', 'Features', 'Train', 'Evaluate'])\n",
    "ax2.set_yticks([])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dask_dag_execution.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTotal pipeline execution time: {dag_df['end_time'].max():.2f} seconds\")\n",
    "print(f\"Sum of individual task times: {dag_df['duration'].sum():.2f} seconds\")\n",
    "print(f\"Parallelization speedup: {dag_df['duration'].sum() / dag_df['end_time'].max():.2f}x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Monitoring and Visualization <a id=\"monitoring\"></a>\n",
    "\n",
    "Demonstrates how to analyze worker utilization and task distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate worker statistics\n",
    "n_workers = 4\n",
    "n_time_points = 50\n",
    "\n",
    "np.random.seed(42)\n",
    "time_points = np.arange(n_time_points)\n",
    "\n",
    "# Generate worker utilization data\n",
    "worker_data = []\n",
    "for t in time_points:\n",
    "    for w in range(n_workers):\n",
    "        # Simulate varying utilization\n",
    "        base_util = 0.6 + 0.2 * np.sin(t / 10 + w)\n",
    "        cpu_util = np.clip(base_util + np.random.normal(0, 0.1), 0, 1)\n",
    "        memory_util = np.clip(0.4 + 0.3 * cpu_util + np.random.normal(0, 0.05), 0, 1)\n",
    "        tasks_running = int(cpu_util * 4)\n",
    "        \n",
    "        worker_data.append({\n",
    "            'time': t,\n",
    "            'worker': f'worker-{w}',\n",
    "            'cpu_utilization': cpu_util,\n",
    "            'memory_utilization': memory_util,\n",
    "            'tasks_running': tasks_running\n",
    "        })\n",
    "\n",
    "worker_df = pd.DataFrame(worker_data)\n",
    "worker_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize worker monitoring data\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. CPU utilization over time by worker\n",
    "ax1 = axes[0, 0]\n",
    "for worker in worker_df['worker'].unique():\n",
    "    data = worker_df[worker_df['worker'] == worker]\n",
    "    ax1.plot(data['time'], data['cpu_utilization'] * 100, label=worker, alpha=0.8)\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('CPU Utilization (%)')\n",
    "ax1.set_title('CPU Utilization Over Time')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.set_ylim(0, 100)\n",
    "\n",
    "# 2. Memory utilization heatmap\n",
    "ax2 = axes[0, 1]\n",
    "pivot_memory = worker_df.pivot(index='worker', columns='time', values='memory_utilization')\n",
    "sns.heatmap(pivot_memory, cmap='YlOrRd', ax=ax2, cbar_kws={'label': 'Memory Utilization'})\n",
    "ax2.set_xlabel('Time')\n",
    "ax2.set_ylabel('Worker')\n",
    "ax2.set_title('Memory Utilization Heatmap')\n",
    "\n",
    "# 3. Average utilization by worker\n",
    "ax3 = axes[1, 0]\n",
    "avg_util = worker_df.groupby('worker').agg({\n",
    "    'cpu_utilization': 'mean',\n",
    "    'memory_utilization': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "x = np.arange(len(avg_util))\n",
    "width = 0.35\n",
    "ax3.bar(x - width/2, avg_util['cpu_utilization'] * 100, width, label='CPU', color='steelblue')\n",
    "ax3.bar(x + width/2, avg_util['memory_utilization'] * 100, width, label='Memory', color='coral')\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(avg_util['worker'])\n",
    "ax3.set_ylabel('Average Utilization (%)')\n",
    "ax3.set_title('Average Resource Utilization by Worker')\n",
    "ax3.legend()\n",
    "ax3.set_ylim(0, 100)\n",
    "\n",
    "# 4. Task distribution over time\n",
    "ax4 = axes[1, 1]\n",
    "total_tasks = worker_df.groupby('time')['tasks_running'].sum()\n",
    "ax4.fill_between(total_tasks.index, total_tasks.values, alpha=0.3, color='green')\n",
    "ax4.plot(total_tasks.index, total_tasks.values, 'g-', linewidth=2)\n",
    "ax4.axhline(total_tasks.mean(), color='red', linestyle='--', \n",
    "            label=f'Mean: {total_tasks.mean():.1f} tasks')\n",
    "ax4.set_xlabel('Time')\n",
    "ax4.set_ylabel('Total Running Tasks')\n",
    "ax4.set_title('Total Tasks Running Over Time')\n",
    "ax4.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('dask_worker_monitoring.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nWorker Statistics Summary:\")\n",
    "print(f\"Average CPU utilization: {worker_df['cpu_utilization'].mean()*100:.1f}%\")\n",
    "print(f\"Average memory utilization: {worker_df['memory_utilization'].mean()*100:.1f}%\")\n",
    "print(f\"Peak concurrent tasks: {worker_df.groupby('time')['tasks_running'].sum().max()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated using QTAU with Dask for:\n",
    "\n",
    "1. **Basic task distribution** with prime counting and computational tasks\n",
    "2. **Parallel data processing** with chunk-based MapReduce patterns\n",
    "3. **Distributed NumPy operations** for matrix computations\n",
    "4. **Task dependencies and DAGs** with complex pipelines\n",
    "5. **Worker monitoring** and resource utilization analysis\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Dask is excellent for data-parallel workloads and complex DAGs\n",
    "- QTAU provides a unified interface for Dask cluster management\n",
    "- Task scheduling respects dependencies automatically\n",
    "- Monitoring helps identify bottlenecks and optimize resource usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
