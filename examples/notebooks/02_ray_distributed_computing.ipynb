{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QTAU with Ray: Distributed Computing Examples\n",
    "\n",
    "This notebook demonstrates how to use QTAU with the **Ray** execution engine for distributed computing tasks.\n",
    "\n",
    "## Overview\n",
    "\n",
    "Ray is a high-performance distributed computing framework that excels at:\n",
    "- Actor-based computations\n",
    "- GPU-accelerated workloads\n",
    "- Large-scale machine learning\n",
    "- Low-latency task execution\n",
    "\n",
    "### Table of Contents\n",
    "1. [Setup and Configuration](#setup)\n",
    "2. [Basic Ray Task Distribution](#basic)\n",
    "3. [Resource-Aware Task Scheduling](#resources)\n",
    "4. [GPU Task Execution](#gpu)\n",
    "5. [Throughput Scaling Analysis](#throughput)\n",
    "6. [Actor Patterns](#actors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration <a id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"Set2\")\n",
    "\n",
    "# QTAU imports\n",
    "from qtau.pilot_compute_service import PilotComputeService, ExecutionEngine\n",
    "\n",
    "print(\"Imports successful!\")\n",
    "print(f\"Current time: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration for Ray execution\n",
    "RESOURCE_URL = \"ssh://localhost\"\n",
    "WORKING_DIRECTORY = os.path.join(os.environ[\"HOME\"], \"work\", \"qtau_ray\")\n",
    "\n",
    "# Create working directory\n",
    "os.makedirs(WORKING_DIRECTORY, exist_ok=True)\n",
    "\n",
    "# Ray pilot configuration\n",
    "pilot_compute_description_ray = {\n",
    "    \"resource\": RESOURCE_URL,\n",
    "    \"working_directory\": WORKING_DIRECTORY,\n",
    "    \"type\": \"ray\",\n",
    "    \"number_of_nodes\": 2,\n",
    "    \"cores_per_node\": 8,\n",
    "    \"gpus_per_node\": 0,  # Set to number of GPUs if available\n",
    "}\n",
    "\n",
    "# Ray with SLURM configuration (for HPC clusters)\n",
    "pilot_compute_description_ray_slurm = {\n",
    "    \"resource\": \"slurm://localhost\",\n",
    "    \"working_directory\": WORKING_DIRECTORY,\n",
    "    \"type\": \"ray\",\n",
    "    \"number_of_nodes\": 4,\n",
    "    \"cores_per_node\": 64,\n",
    "    \"gpus_per_node\": 4,\n",
    "    \"queue\": \"regular\",\n",
    "    \"walltime\": 30,\n",
    "    \"project\": \"myproject\",\n",
    "    \"scheduler_script_commands\": [\n",
    "        \"#SBATCH --constraint=gpu\",\n",
    "        \"#SBATCH --gpus-per-task=1\"\n",
    "    ]\n",
    "}\n",
    "\n",
    "print(\"Ray Configuration:\")\n",
    "for key, value in pilot_compute_description_ray.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Starting QTAU with Ray (Production Code)\n",
    "\n",
    "```python\n",
    "# Initialize PilotComputeService with Ray engine\n",
    "pcs = PilotComputeService(\n",
    "    execution_engine=ExecutionEngine.RAY,\n",
    "    working_directory=WORKING_DIRECTORY\n",
    ")\n",
    "\n",
    "# Create pilot\n",
    "pilot = pcs.create_pilot(pilot_compute_description=pilot_compute_description_ray)\n",
    "pilot.wait()\n",
    "\n",
    "# Submit tasks with resource requirements\n",
    "task = pcs.submit_task(\n",
    "    my_function, \n",
    "    arg1, arg2,\n",
    "    resources={'num_cpus': 1, 'num_gpus': 0, 'memory': None}\n",
    ")\n",
    "\n",
    "# Get results\n",
    "pcs.wait_tasks([task])\n",
    "result = pcs.get_results([task])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Ray Task Distribution <a id=\"basic\"></a>\n",
    "\n",
    "Demonstrates basic task submission patterns with Ray."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define computational tasks\n",
    "def monte_carlo_pi(n_samples):\n",
    "    \"\"\"Estimate Pi using Monte Carlo method.\"\"\"\n",
    "    x = np.random.uniform(-1, 1, n_samples)\n",
    "    y = np.random.uniform(-1, 1, n_samples)\n",
    "    inside_circle = np.sum(x**2 + y**2 <= 1)\n",
    "    return 4 * inside_circle / n_samples\n",
    "\n",
    "def numerical_integration(func, a, b, n_points):\n",
    "    \"\"\"Numerical integration using trapezoidal rule.\"\"\"\n",
    "    x = np.linspace(a, b, n_points)\n",
    "    y = func(x)\n",
    "    return np.trapz(y, x)\n",
    "\n",
    "def simulate_random_walk(n_steps, n_walkers):\n",
    "    \"\"\"Simulate multiple random walks.\"\"\"\n",
    "    walks = np.cumsum(np.random.choice([-1, 1], size=(n_walkers, n_steps)), axis=1)\n",
    "    return {\n",
    "        'final_positions': walks[:, -1].tolist(),\n",
    "        'max_displacement': float(np.max(np.abs(walks))),\n",
    "        'mean_squared_displacement': float(np.mean(walks[:, -1]**2))\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monte Carlo Pi estimation with increasing samples\n",
    "np.random.seed(42)\n",
    "sample_sizes = [1000, 10000, 100000, 500000, 1000000, 5000000]\n",
    "n_trials = 10\n",
    "\n",
    "pi_results = []\n",
    "for n_samples in sample_sizes:\n",
    "    estimates = []\n",
    "    times = []\n",
    "    for _ in range(n_trials):\n",
    "        start = time.time()\n",
    "        pi_est = monte_carlo_pi(n_samples)\n",
    "        elapsed = time.time() - start\n",
    "        estimates.append(pi_est)\n",
    "        times.append(elapsed)\n",
    "    \n",
    "    pi_results.append({\n",
    "        'n_samples': n_samples,\n",
    "        'pi_estimate': np.mean(estimates),\n",
    "        'std_error': np.std(estimates),\n",
    "        'error': abs(np.mean(estimates) - np.pi),\n",
    "        'mean_time': np.mean(times),\n",
    "        'samples_per_sec': n_samples / np.mean(times)\n",
    "    })\n",
    "\n",
    "pi_df = pd.DataFrame(pi_results)\n",
    "pi_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize Monte Carlo Pi estimation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Pi estimate convergence\n",
    "ax1 = axes[0, 0]\n",
    "ax1.errorbar(pi_df['n_samples'], pi_df['pi_estimate'], yerr=pi_df['std_error'],\n",
    "             fmt='o-', capsize=5, linewidth=2, markersize=8, color='steelblue')\n",
    "ax1.axhline(np.pi, color='red', linestyle='--', label=f'True π = {np.pi:.6f}')\n",
    "ax1.set_xscale('log')\n",
    "ax1.set_xlabel('Number of Samples')\n",
    "ax1.set_ylabel('Estimated π')\n",
    "ax1.set_title('Monte Carlo π Estimation Convergence')\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Error vs samples (log-log)\n",
    "ax2 = axes[0, 1]\n",
    "ax2.loglog(pi_df['n_samples'], pi_df['error'], 'go-', linewidth=2, markersize=8, label='Actual Error')\n",
    "# Theoretical 1/sqrt(n) convergence\n",
    "n_ref = np.array(sample_sizes)\n",
    "theoretical_error = 1.0 / np.sqrt(n_ref) * pi_df['error'].iloc[0] * np.sqrt(sample_sizes[0])\n",
    "ax2.loglog(n_ref, theoretical_error, 'r--', alpha=0.7, label='1/√n theoretical')\n",
    "ax2.set_xlabel('Number of Samples')\n",
    "ax2.set_ylabel('Absolute Error')\n",
    "ax2.set_title('Error Convergence Rate')\n",
    "ax2.legend()\n",
    "ax2.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "\n",
    "# 3. Throughput\n",
    "ax3 = axes[1, 0]\n",
    "ax3.bar(range(len(pi_df)), pi_df['samples_per_sec'] / 1e6, color='coral')\n",
    "ax3.set_xticks(range(len(pi_df)))\n",
    "ax3.set_xticklabels([f'{n/1e6:.1f}M' if n >= 1e6 else f'{n/1e3:.0f}K' \n",
    "                     for n in pi_df['n_samples']], rotation=45)\n",
    "ax3.set_xlabel('Sample Size')\n",
    "ax3.set_ylabel('Throughput (Million samples/sec)')\n",
    "ax3.set_title('Computational Throughput')\n",
    "\n",
    "# 4. Execution time\n",
    "ax4 = axes[1, 1]\n",
    "ax4.semilogy(pi_df['n_samples'], pi_df['mean_time'], 'mo-', linewidth=2, markersize=8)\n",
    "ax4.set_xscale('log')\n",
    "ax4.set_xlabel('Number of Samples')\n",
    "ax4.set_ylabel('Execution Time (seconds)')\n",
    "ax4.set_title('Execution Time Scaling')\n",
    "ax4.grid(True, which=\"both\", ls=\"-\", alpha=0.2)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ray_monte_carlo_pi.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nBest π estimate: {pi_df.iloc[-1]['pi_estimate']:.6f} (error: {pi_df.iloc[-1]['error']:.6f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Resource-Aware Task Scheduling <a id=\"resources\"></a>\n",
    "\n",
    "Ray allows fine-grained resource allocation per task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate resource-aware task scheduling\n",
    "# Different tasks with different resource requirements\n",
    "\n",
    "task_types = [\n",
    "    {'name': 'light_cpu', 'cpus': 1, 'memory_gb': 1, 'duration': 0.5},\n",
    "    {'name': 'heavy_cpu', 'cpus': 4, 'memory_gb': 2, 'duration': 2.0},\n",
    "    {'name': 'memory_intensive', 'cpus': 2, 'memory_gb': 8, 'duration': 1.5},\n",
    "    {'name': 'gpu_task', 'cpus': 1, 'gpus': 1, 'memory_gb': 4, 'duration': 1.0},\n",
    "]\n",
    "\n",
    "# Simulate scheduling on a cluster with 16 CPUs, 32GB RAM, 2 GPUs\n",
    "cluster_resources = {'cpus': 16, 'memory_gb': 32, 'gpus': 2}\n",
    "\n",
    "# Generate task schedule\n",
    "np.random.seed(42)\n",
    "n_tasks = 50\n",
    "\n",
    "scheduled_tasks = []\n",
    "current_time = 0\n",
    "resource_timeline = []\n",
    "\n",
    "for i in range(n_tasks):\n",
    "    task_type = task_types[np.random.randint(0, len(task_types))]\n",
    "    \n",
    "    # Add some randomness to duration\n",
    "    actual_duration = task_type['duration'] * (0.8 + 0.4 * np.random.random())\n",
    "    \n",
    "    scheduled_tasks.append({\n",
    "        'task_id': f'task_{i}',\n",
    "        'type': task_type['name'],\n",
    "        'cpus': task_type['cpus'],\n",
    "        'memory_gb': task_type.get('memory_gb', 0),\n",
    "        'gpus': task_type.get('gpus', 0),\n",
    "        'duration': actual_duration,\n",
    "        'start_time': current_time + np.random.exponential(0.2),\n",
    "    })\n",
    "    current_time = scheduled_tasks[-1]['start_time']\n",
    "\n",
    "tasks_df = pd.DataFrame(scheduled_tasks)\n",
    "tasks_df['end_time'] = tasks_df['start_time'] + tasks_df['duration']\n",
    "tasks_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize resource-aware scheduling\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Task type distribution\n",
    "ax1 = axes[0, 0]\n",
    "type_counts = tasks_df['type'].value_counts()\n",
    "colors = plt.cm.Set2(range(len(type_counts)))\n",
    "ax1.pie(type_counts.values, labels=type_counts.index, autopct='%1.1f%%',\n",
    "        colors=colors, explode=[0.02]*len(type_counts))\n",
    "ax1.set_title('Task Type Distribution')\n",
    "\n",
    "# 2. Resource requirements by task type\n",
    "ax2 = axes[0, 1]\n",
    "resource_summary = tasks_df.groupby('type').agg({\n",
    "    'cpus': 'mean',\n",
    "    'memory_gb': 'mean',\n",
    "    'gpus': 'mean',\n",
    "    'duration': 'mean'\n",
    "}).reset_index()\n",
    "\n",
    "x = np.arange(len(resource_summary))\n",
    "width = 0.25\n",
    "ax2.bar(x - width, resource_summary['cpus'], width, label='CPUs', color='steelblue')\n",
    "ax2.bar(x, resource_summary['memory_gb'], width, label='Memory (GB)', color='coral')\n",
    "ax2.bar(x + width, resource_summary['gpus'] * 4, width, label='GPUs (×4)', color='green')\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(resource_summary['type'], rotation=45, ha='right')\n",
    "ax2.set_ylabel('Resource Units')\n",
    "ax2.set_title('Average Resource Requirements by Task Type')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Task execution timeline (Gantt chart)\n",
    "ax3 = axes[1, 0]\n",
    "type_colors = {'light_cpu': 'lightblue', 'heavy_cpu': 'steelblue', \n",
    "               'memory_intensive': 'coral', 'gpu_task': 'lightgreen'}\n",
    "\n",
    "# Show first 20 tasks for clarity\n",
    "for i, (_, task) in enumerate(tasks_df.head(20).iterrows()):\n",
    "    ax3.barh(i, task['duration'], left=task['start_time'],\n",
    "             color=type_colors[task['type']], edgecolor='black', linewidth=0.5)\n",
    "\n",
    "ax3.set_yticks(range(20))\n",
    "ax3.set_yticklabels([f\"{t['type'][:6]}\" for _, t in tasks_df.head(20).iterrows()], fontsize=8)\n",
    "ax3.set_xlabel('Time (seconds)')\n",
    "ax3.set_ylabel('Task')\n",
    "ax3.set_title('Task Execution Timeline (First 20 Tasks)')\n",
    "\n",
    "# Add legend\n",
    "from matplotlib.patches import Patch\n",
    "legend_elements = [Patch(facecolor=c, label=t) for t, c in type_colors.items()]\n",
    "ax3.legend(handles=legend_elements, loc='lower right', fontsize=8)\n",
    "\n",
    "# 4. Duration distribution by type\n",
    "ax4 = axes[1, 1]\n",
    "tasks_df.boxplot(column='duration', by='type', ax=ax4)\n",
    "ax4.set_xlabel('Task Type')\n",
    "ax4.set_ylabel('Duration (seconds)')\n",
    "ax4.set_title('Task Duration Distribution by Type')\n",
    "plt.suptitle('')  # Remove automatic title\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ray_resource_scheduling.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nResource Summary:\")\n",
    "print(f\"Total CPU-seconds: {(tasks_df['cpus'] * tasks_df['duration']).sum():.1f}\")\n",
    "print(f\"Total GPU-seconds: {(tasks_df['gpus'] * tasks_df['duration']).sum():.1f}\")\n",
    "print(f\"Total memory-GB-seconds: {(tasks_df['memory_gb'] * tasks_df['duration']).sum():.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. GPU Task Execution <a id=\"gpu\"></a>\n",
    "\n",
    "Ray with QTAU enables easy GPU task distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate GPU vs CPU performance comparison\n",
    "# In practice, you would use actual GPU computations\n",
    "\n",
    "def simulate_gpu_speedup(operation, size):\n",
    "    \"\"\"Simulate GPU speedup for different operations.\"\"\"\n",
    "    # Simulated speedup factors based on typical GPU performance\n",
    "    speedup_factors = {\n",
    "        'matrix_multiply': 20 + 5 * np.log10(size/100),\n",
    "        'fft': 15 + 3 * np.log10(size/100),\n",
    "        'neural_network': 50 + 10 * np.log10(size/100),\n",
    "        'monte_carlo': 30 + 8 * np.log10(size/100),\n",
    "    }\n",
    "    return max(1, speedup_factors.get(operation, 10))\n",
    "\n",
    "# Generate comparison data\n",
    "operations = ['matrix_multiply', 'fft', 'neural_network', 'monte_carlo']\n",
    "sizes = [100, 500, 1000, 2000, 5000, 10000]\n",
    "\n",
    "gpu_comparison = []\n",
    "for op in operations:\n",
    "    for size in sizes:\n",
    "        cpu_time = size**2 / 1e8  # Simplified CPU time model\n",
    "        speedup = simulate_gpu_speedup(op, size)\n",
    "        gpu_time = cpu_time / speedup\n",
    "        \n",
    "        gpu_comparison.append({\n",
    "            'operation': op,\n",
    "            'size': size,\n",
    "            'cpu_time': cpu_time,\n",
    "            'gpu_time': gpu_time,\n",
    "            'speedup': speedup\n",
    "        })\n",
    "\n",
    "gpu_df = pd.DataFrame(gpu_comparison)\n",
    "gpu_df.head(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize GPU performance\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Speedup by operation and size\n",
    "ax1 = axes[0, 0]\n",
    "for op in operations:\n",
    "    op_data = gpu_df[gpu_df['operation'] == op]\n",
    "    ax1.plot(op_data['size'], op_data['speedup'], 'o-', label=op.replace('_', ' ').title(),\n",
    "             linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Problem Size')\n",
    "ax1.set_ylabel('GPU Speedup (×)')\n",
    "ax1.set_title('GPU Speedup by Operation Type')\n",
    "ax1.legend()\n",
    "ax1.set_xscale('log')\n",
    "\n",
    "# 2. CPU vs GPU time comparison\n",
    "ax2 = axes[0, 1]\n",
    "pivot_cpu = gpu_df.pivot(index='size', columns='operation', values='cpu_time')\n",
    "pivot_gpu = gpu_df.pivot(index='size', columns='operation', values='gpu_time')\n",
    "\n",
    "x = np.arange(len(sizes))\n",
    "width = 0.4\n",
    "ax2.bar(x - width/2, pivot_cpu['matrix_multiply'], width, label='CPU', color='steelblue', alpha=0.8)\n",
    "ax2.bar(x + width/2, pivot_gpu['matrix_multiply'], width, label='GPU', color='green', alpha=0.8)\n",
    "ax2.set_xticks(x)\n",
    "ax2.set_xticklabels(sizes)\n",
    "ax2.set_xlabel('Problem Size')\n",
    "ax2.set_ylabel('Execution Time (seconds)')\n",
    "ax2.set_title('CPU vs GPU: Matrix Multiplication')\n",
    "ax2.legend()\n",
    "ax2.set_yscale('log')\n",
    "\n",
    "# 3. Speedup heatmap\n",
    "ax3 = axes[1, 0]\n",
    "pivot_speedup = gpu_df.pivot(index='operation', columns='size', values='speedup')\n",
    "sns.heatmap(pivot_speedup, annot=True, fmt='.1f', cmap='YlGnBu', ax=ax3,\n",
    "            cbar_kws={'label': 'Speedup (×)'})\n",
    "ax3.set_xlabel('Problem Size')\n",
    "ax3.set_ylabel('Operation')\n",
    "ax3.set_title('GPU Speedup Heatmap')\n",
    "\n",
    "# 4. Time savings\n",
    "ax4 = axes[1, 1]\n",
    "gpu_df['time_saved'] = gpu_df['cpu_time'] - gpu_df['gpu_time']\n",
    "gpu_df['pct_saved'] = (gpu_df['time_saved'] / gpu_df['cpu_time']) * 100\n",
    "\n",
    "for op in operations:\n",
    "    op_data = gpu_df[gpu_df['operation'] == op]\n",
    "    ax4.plot(op_data['size'], op_data['pct_saved'], 'o-', label=op.replace('_', ' ').title(),\n",
    "             linewidth=2, markersize=8)\n",
    "\n",
    "ax4.set_xlabel('Problem Size')\n",
    "ax4.set_ylabel('Time Saved (%)')\n",
    "ax4.set_title('Percentage of Time Saved with GPU')\n",
    "ax4.legend()\n",
    "ax4.set_xscale('log')\n",
    "ax4.set_ylim(90, 100)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ray_gpu_performance.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nGPU Performance Summary:\")\n",
    "print(f\"Average speedup: {gpu_df['speedup'].mean():.1f}×\")\n",
    "print(f\"Max speedup: {gpu_df['speedup'].max():.1f}× ({gpu_df.loc[gpu_df['speedup'].idxmax(), 'operation']})\")\n",
    "print(f\"Average time saved: {gpu_df['pct_saved'].mean():.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Throughput Scaling Analysis <a id=\"throughput\"></a>\n",
    "\n",
    "Analyze how throughput scales with cluster size and task count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate throughput benchmark data\n",
    "# Similar to pilot_ray_slurm_throughput.py\n",
    "\n",
    "node_configs = [1, 2, 4, 8]\n",
    "cores_per_node = 64\n",
    "task_counts = [256, 512, 1024, 2048, 4096, 8192]\n",
    "\n",
    "throughput_results = []\n",
    "\n",
    "for nodes in node_configs:\n",
    "    total_cores = nodes * cores_per_node\n",
    "    for n_tasks in task_counts:\n",
    "        # Simulate execution time with some overhead\n",
    "        task_time = 0.001  # 1ms per task\n",
    "        scheduling_overhead = 0.1 * np.log(n_tasks)  # Scheduling overhead\n",
    "        communication_overhead = 0.05 * nodes  # Communication overhead\n",
    "        \n",
    "        # Parallel execution time\n",
    "        parallel_batches = np.ceil(n_tasks / total_cores)\n",
    "        execution_time = parallel_batches * task_time + scheduling_overhead + communication_overhead\n",
    "        \n",
    "        # Add some noise\n",
    "        execution_time *= (0.9 + 0.2 * np.random.random())\n",
    "        \n",
    "        throughput = n_tasks / execution_time\n",
    "        efficiency = throughput / total_cores\n",
    "        \n",
    "        throughput_results.append({\n",
    "            'nodes': nodes,\n",
    "            'total_cores': total_cores,\n",
    "            'n_tasks': n_tasks,\n",
    "            'execution_time': execution_time,\n",
    "            'throughput': throughput,\n",
    "            'efficiency': efficiency,\n",
    "            'tasks_per_core': n_tasks / total_cores\n",
    "        })\n",
    "\n",
    "throughput_df = pd.DataFrame(throughput_results)\n",
    "throughput_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize throughput scaling\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Throughput vs task count by node count\n",
    "ax1 = axes[0, 0]\n",
    "for nodes in node_configs:\n",
    "    data = throughput_df[throughput_df['nodes'] == nodes]\n",
    "    ax1.plot(data['n_tasks'], data['throughput'], 'o-', \n",
    "             label=f'{nodes} nodes ({nodes*64} cores)', linewidth=2, markersize=8)\n",
    "ax1.set_xlabel('Number of Tasks')\n",
    "ax1.set_ylabel('Throughput (tasks/second)')\n",
    "ax1.set_title('Throughput Scaling with Task Count')\n",
    "ax1.legend()\n",
    "ax1.set_xscale('log', base=2)\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# 2. Strong scaling (fixed problem, more resources)\n",
    "ax2 = axes[0, 1]\n",
    "fixed_tasks = 4096\n",
    "strong_data = throughput_df[throughput_df['n_tasks'] == fixed_tasks]\n",
    "ax2.plot(strong_data['total_cores'], strong_data['throughput'], 'go-', \n",
    "         linewidth=2, markersize=10, label='Actual')\n",
    "\n",
    "# Ideal scaling line\n",
    "ideal_scaling = strong_data['throughput'].iloc[0] * strong_data['total_cores'] / strong_data['total_cores'].iloc[0]\n",
    "ax2.plot(strong_data['total_cores'], ideal_scaling, 'r--', linewidth=2, label='Ideal Linear')\n",
    "\n",
    "ax2.set_xlabel('Total Cores')\n",
    "ax2.set_ylabel('Throughput (tasks/second)')\n",
    "ax2.set_title(f'Strong Scaling ({fixed_tasks} tasks)')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Weak scaling (tasks proportional to resources)\n",
    "ax3 = axes[1, 0]\n",
    "# Select entries where tasks_per_core is approximately constant\n",
    "weak_data = throughput_df[throughput_df['tasks_per_core'].between(15, 17)]\n",
    "if len(weak_data) > 0:\n",
    "    ax3.plot(weak_data['total_cores'], weak_data['efficiency'], 'bo-', \n",
    "             linewidth=2, markersize=10)\n",
    "else:\n",
    "    # Fallback: show efficiency for fixed task count\n",
    "    ax3.plot(strong_data['total_cores'], strong_data['efficiency'], 'bo-', \n",
    "             linewidth=2, markersize=10)\n",
    "ax3.axhline(1.0, color='red', linestyle='--', alpha=0.7, label='Ideal efficiency')\n",
    "ax3.set_xlabel('Total Cores')\n",
    "ax3.set_ylabel('Efficiency (throughput/core)')\n",
    "ax3.set_title('Parallel Efficiency')\n",
    "ax3.legend()\n",
    "\n",
    "# 4. Execution time heatmap\n",
    "ax4 = axes[1, 1]\n",
    "pivot_time = throughput_df.pivot(index='nodes', columns='n_tasks', values='execution_time')\n",
    "sns.heatmap(pivot_time, annot=True, fmt='.2f', cmap='RdYlGn_r', ax=ax4,\n",
    "            cbar_kws={'label': 'Execution Time (s)'})\n",
    "ax4.set_xlabel('Number of Tasks')\n",
    "ax4.set_ylabel('Number of Nodes')\n",
    "ax4.set_title('Execution Time (seconds)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ray_throughput_scaling.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nScaling Summary:\")\n",
    "print(f\"Max throughput: {throughput_df['throughput'].max():.0f} tasks/second\")\n",
    "print(f\"Best config: {throughput_df.loc[throughput_df['throughput'].idxmax(), 'nodes']} nodes, \"\n",
    "      f\"{throughput_df.loc[throughput_df['throughput'].idxmax(), 'n_tasks']} tasks\")\n",
    "print(f\"Average efficiency: {throughput_df['efficiency'].mean():.2f} tasks/second/core\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Actor Patterns <a id=\"actors\"></a>\n",
    "\n",
    "Ray supports actor-based computation patterns for stateful distributed computing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate actor-based parameter server pattern\n",
    "# Common in distributed machine learning\n",
    "\n",
    "class SimulatedParameterServer:\n",
    "    \"\"\"Simulates a parameter server actor.\"\"\"\n",
    "    def __init__(self, n_params):\n",
    "        self.params = np.random.randn(n_params)\n",
    "        self.update_count = 0\n",
    "        self.history = []\n",
    "    \n",
    "    def get_params(self):\n",
    "        return self.params.copy()\n",
    "    \n",
    "    def apply_gradients(self, gradients, learning_rate=0.01):\n",
    "        self.params -= learning_rate * gradients\n",
    "        self.update_count += 1\n",
    "        self.history.append(np.linalg.norm(self.params))\n",
    "        return self.params.copy()\n",
    "\n",
    "def simulate_worker_training(worker_id, n_iterations, batch_size):\n",
    "    \"\"\"Simulate worker computing gradients.\"\"\"\n",
    "    results = []\n",
    "    for i in range(n_iterations):\n",
    "        # Simulate gradient computation\n",
    "        gradient = np.random.randn(100) * 0.1\n",
    "        loss = np.random.exponential(1.0 / (i + 1))\n",
    "        results.append({\n",
    "            'worker_id': worker_id,\n",
    "            'iteration': i,\n",
    "            'loss': loss,\n",
    "            'gradient_norm': np.linalg.norm(gradient)\n",
    "        })\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate distributed training\n",
    "n_workers = 4\n",
    "n_iterations = 50\n",
    "batch_size = 32\n",
    "\n",
    "# Run workers\n",
    "all_results = []\n",
    "for worker_id in range(n_workers):\n",
    "    np.random.seed(worker_id * 42)\n",
    "    results = simulate_worker_training(worker_id, n_iterations, batch_size)\n",
    "    all_results.extend(results)\n",
    "\n",
    "training_df = pd.DataFrame(all_results)\n",
    "\n",
    "# Simulate parameter server\n",
    "ps = SimulatedParameterServer(100)\n",
    "for _ in range(n_iterations * n_workers):\n",
    "    gradients = np.random.randn(100) * 0.1\n",
    "    ps.apply_gradients(gradients)\n",
    "\n",
    "training_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize distributed training\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Loss curves per worker\n",
    "ax1 = axes[0, 0]\n",
    "for worker_id in range(n_workers):\n",
    "    worker_data = training_df[training_df['worker_id'] == worker_id]\n",
    "    ax1.plot(worker_data['iteration'], worker_data['loss'], \n",
    "             label=f'Worker {worker_id}', alpha=0.7, linewidth=2)\n",
    "\n",
    "# Average loss\n",
    "avg_loss = training_df.groupby('iteration')['loss'].mean()\n",
    "ax1.plot(avg_loss.index, avg_loss.values, 'k-', linewidth=3, label='Average')\n",
    "\n",
    "ax1.set_xlabel('Iteration')\n",
    "ax1.set_ylabel('Loss')\n",
    "ax1.set_title('Training Loss by Worker')\n",
    "ax1.legend()\n",
    "ax1.set_yscale('log')\n",
    "\n",
    "# 2. Parameter norm evolution\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(ps.history, 'b-', linewidth=2)\n",
    "ax2.set_xlabel('Update Step')\n",
    "ax2.set_ylabel('Parameter Norm')\n",
    "ax2.set_title('Parameter Server: Parameter Norm Evolution')\n",
    "ax2.axhline(ps.history[0], color='red', linestyle='--', alpha=0.5, label='Initial')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Gradient norm distribution\n",
    "ax3 = axes[1, 0]\n",
    "training_df.boxplot(column='gradient_norm', by='worker_id', ax=ax3)\n",
    "ax3.set_xlabel('Worker ID')\n",
    "ax3.set_ylabel('Gradient Norm')\n",
    "ax3.set_title('Gradient Norm Distribution by Worker')\n",
    "plt.suptitle('')\n",
    "\n",
    "# 4. Work distribution pie chart\n",
    "ax4 = axes[1, 1]\n",
    "work_per_worker = training_df.groupby('worker_id').size()\n",
    "ax4.pie(work_per_worker.values, labels=[f'Worker {i}' for i in work_per_worker.index],\n",
    "        autopct='%1.1f%%', colors=plt.cm.Set2(range(n_workers)))\n",
    "ax4.set_title('Work Distribution Across Workers')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('ray_actor_training.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nDistributed Training Summary:\")\n",
    "print(f\"Total iterations: {len(training_df)}\")\n",
    "print(f\"Workers: {n_workers}\")\n",
    "print(f\"Final average loss: {training_df.groupby('iteration')['loss'].mean().iloc[-1]:.4f}\")\n",
    "print(f\"Parameter updates: {ps.update_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated using QTAU with Ray for:\n",
    "\n",
    "1. **Basic task distribution** with Monte Carlo simulations\n",
    "2. **Resource-aware scheduling** with CPU, memory, and GPU requirements\n",
    "3. **GPU task execution** comparing CPU vs GPU performance\n",
    "4. **Throughput scaling analysis** with strong and weak scaling\n",
    "5. **Actor patterns** for stateful distributed computing\n",
    "\n",
    "### Key Takeaways\n",
    "\n",
    "- Ray excels at low-latency task execution and GPU workloads\n",
    "- Resource specifications allow fine-grained control over task placement\n",
    "- Actor patterns enable stateful distributed applications\n",
    "- QTAU simplifies Ray cluster management on HPC systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
