{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QTAU - Quantum Task Automation Utility\n",
    "## Visualization Examples\n",
    "\n",
    "This notebook demonstrates various use cases of QTAU (Quantum-HPC middleware framework) with result visualizations.\n",
    "\n",
    "QTAU provides a unified interface for managing heterogeneous computational resources, bridging quantum computing and classical HPC systems.\n",
    "\n",
    "### Table of Contents\n",
    "1. [Setup and Installation](#setup)\n",
    "2. [Basic Task Distribution](#basic-tasks)\n",
    "3. [PennyLane Quantum Circuit Execution](#pennylane)\n",
    "4. [Throughput Benchmarking](#throughput)\n",
    "5. [Metrics Analysis](#metrics)\n",
    "6. [Hybrid Quantum-Classical Optimization](#hybrid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Installation <a id=\"setup\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# QTAU imports\n",
    "from qtau.pilot_compute_service import PilotComputeService, ExecutionEngine\n",
    "\n",
    "# Quantum computing imports\n",
    "import pennylane as qml\n",
    "from pennylane import numpy as pnp\n",
    "\n",
    "print(\"All imports successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "RESOURCE_URL = \"ssh://localhost\"  # Use SSH for local execution\n",
    "WORKING_DIRECTORY = os.path.join(os.environ[\"HOME\"], \"work\", \"qtau_notebook\")\n",
    "\n",
    "# Create working directory if it doesn't exist\n",
    "os.makedirs(WORKING_DIRECTORY, exist_ok=True)\n",
    "\n",
    "print(f\"Working directory: {WORKING_DIRECTORY}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Basic Task Distribution <a id=\"basic-tasks\"></a>\n",
    "\n",
    "This example demonstrates distributing simple computational tasks across multiple workers and visualizing the execution times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define pilot configuration for Dask execution engine\n",
    "pilot_compute_description_dask = {\n",
    "    \"resource\": RESOURCE_URL,\n",
    "    \"working_directory\": WORKING_DIRECTORY,\n",
    "    \"type\": \"dask\",\n",
    "    \"number_of_nodes\": 1,\n",
    "    \"cores_per_node\": 4,\n",
    "}\n",
    "\n",
    "# Define pilot configuration for Ray execution engine\n",
    "pilot_compute_description_ray = {\n",
    "    \"resource\": RESOURCE_URL,\n",
    "    \"working_directory\": WORKING_DIRECTORY,\n",
    "    \"type\": \"ray\",\n",
    "    \"number_of_nodes\": 1,\n",
    "    \"cores_per_node\": 4,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define computational tasks\n",
    "def compute_square(x):\n",
    "    \"\"\"Compute square of a number with simulated delay.\"\"\"\n",
    "    time.sleep(0.1)  # Simulate computation time\n",
    "    return x ** 2\n",
    "\n",
    "def compute_fibonacci(n):\n",
    "    \"\"\"Compute nth Fibonacci number.\"\"\"\n",
    "    if n <= 1:\n",
    "        return n\n",
    "    a, b = 0, 1\n",
    "    for _ in range(2, n + 1):\n",
    "        a, b = b, a + b\n",
    "    return b\n",
    "\n",
    "def matrix_multiply(size):\n",
    "    \"\"\"Multiply two random matrices of given size.\"\"\"\n",
    "    A = np.random.rand(size, size)\n",
    "    B = np.random.rand(size, size)\n",
    "    return np.dot(A, B).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Run tasks and collect timing data\n",
    "# Note: This is a simulation since we can't run actual distributed tasks in the notebook\n",
    "# In production, you would use the PilotComputeService as shown in the actual examples\n",
    "\n",
    "# Simulated execution times for visualization\n",
    "np.random.seed(42)\n",
    "n_tasks = 20\n",
    "\n",
    "# Simulate distributed execution times\n",
    "task_ids = [f\"task-{i}\" for i in range(n_tasks)]\n",
    "submit_times = np.cumsum(np.random.exponential(0.05, n_tasks))\n",
    "wait_times = np.random.exponential(0.1, n_tasks)\n",
    "execution_times = np.random.exponential(0.5, n_tasks)\n",
    "completion_times = submit_times + wait_times + execution_times\n",
    "\n",
    "# Create a DataFrame for analysis\n",
    "task_df = pd.DataFrame({\n",
    "    'task_id': task_ids,\n",
    "    'submit_time': submit_times,\n",
    "    'wait_time': wait_times,\n",
    "    'execution_time': execution_times,\n",
    "    'completion_time': completion_times,\n",
    "    'status': ['SUCCESS'] * n_tasks\n",
    "})\n",
    "\n",
    "task_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize task execution timeline\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Task Timeline (Gantt-style chart)\n",
    "ax1 = axes[0, 0]\n",
    "colors = plt.cm.viridis(np.linspace(0, 1, n_tasks))\n",
    "for i, (_, row) in enumerate(task_df.iterrows()):\n",
    "    # Wait time (lighter color)\n",
    "    ax1.barh(i, row['wait_time'], left=row['submit_time'], \n",
    "             color=colors[i], alpha=0.3, label='Wait' if i == 0 else '')\n",
    "    # Execution time (full color)\n",
    "    ax1.barh(i, row['execution_time'], left=row['submit_time'] + row['wait_time'], \n",
    "             color=colors[i], alpha=0.9, label='Execute' if i == 0 else '')\n",
    "\n",
    "ax1.set_xlabel('Time (seconds)')\n",
    "ax1.set_ylabel('Task Index')\n",
    "ax1.set_title('Task Execution Timeline')\n",
    "ax1.legend(['Wait Time', 'Execution Time'], loc='lower right')\n",
    "\n",
    "# 2. Execution Time Distribution\n",
    "ax2 = axes[0, 1]\n",
    "ax2.hist(task_df['execution_time'], bins=10, color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax2.axvline(task_df['execution_time'].mean(), color='red', linestyle='--', \n",
    "            label=f'Mean: {task_df[\"execution_time\"].mean():.3f}s')\n",
    "ax2.set_xlabel('Execution Time (seconds)')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Execution Time Distribution')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Wait Time vs Execution Time\n",
    "ax3 = axes[1, 0]\n",
    "scatter = ax3.scatter(task_df['wait_time'], task_df['execution_time'], \n",
    "                      c=task_df.index, cmap='viridis', s=100, alpha=0.7)\n",
    "ax3.set_xlabel('Wait Time (seconds)')\n",
    "ax3.set_ylabel('Execution Time (seconds)')\n",
    "ax3.set_title('Wait Time vs Execution Time')\n",
    "plt.colorbar(scatter, ax=ax3, label='Task Order')\n",
    "\n",
    "# 4. Cumulative Completion\n",
    "ax4 = axes[1, 1]\n",
    "sorted_completion = np.sort(task_df['completion_time'])\n",
    "ax4.step(sorted_completion, np.arange(1, n_tasks + 1), where='post', \n",
    "         color='green', linewidth=2)\n",
    "ax4.fill_between(sorted_completion, np.arange(1, n_tasks + 1), \n",
    "                 step='post', alpha=0.3, color='green')\n",
    "ax4.set_xlabel('Time (seconds)')\n",
    "ax4.set_ylabel('Completed Tasks')\n",
    "ax4.set_title('Cumulative Task Completion')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('task_execution_visualization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nSummary Statistics:\")\n",
    "print(f\"Total tasks: {n_tasks}\")\n",
    "print(f\"Average wait time: {task_df['wait_time'].mean():.3f}s\")\n",
    "print(f\"Average execution time: {task_df['execution_time'].mean():.3f}s\")\n",
    "print(f\"Total runtime: {task_df['completion_time'].max():.3f}s\")\n",
    "print(f\"Throughput: {n_tasks / task_df['completion_time'].max():.2f} tasks/second\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. PennyLane Quantum Circuit Execution <a id=\"pennylane\"></a>\n",
    "\n",
    "This example demonstrates executing PennyLane quantum circuits and visualizing the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a parameterized quantum circuit\n",
    "n_wires = 4\n",
    "n_layers = 2\n",
    "\n",
    "dev = qml.device('default.qubit', wires=n_wires, shots=None)\n",
    "\n",
    "@qml.qnode(dev)\n",
    "def quantum_circuit(params):\n",
    "    \"\"\"Strongly entangling layers circuit.\"\"\"\n",
    "    qml.StronglyEntanglingLayers(weights=params, wires=range(n_wires))\n",
    "    return [qml.expval(qml.PauliZ(i)) for i in range(n_wires)]\n",
    "\n",
    "# Generate random parameters\n",
    "shape = qml.StronglyEntanglingLayers.shape(n_layers=n_layers, n_wires=n_wires)\n",
    "np.random.seed(42)\n",
    "weights = pnp.random.random(size=shape, requires_grad=True)\n",
    "\n",
    "print(f\"Circuit parameters shape: {shape}\")\n",
    "print(f\"Total parameters: {np.prod(shape)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Execute circuit with different parameter sets and collect results\n",
    "n_samples = 50\n",
    "results = []\n",
    "\n",
    "for i in range(n_samples):\n",
    "    # Generate random parameters for each sample\n",
    "    params = pnp.random.random(size=shape)\n",
    "    expectation_values = quantum_circuit(params)\n",
    "    results.append({\n",
    "        'sample': i,\n",
    "        **{f'Z{j}': float(expectation_values[j]) for j in range(n_wires)}\n",
    "    })\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize quantum circuit results\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Expectation values over samples\n",
    "ax1 = axes[0, 0]\n",
    "for j in range(n_wires):\n",
    "    ax1.plot(results_df['sample'], results_df[f'Z{j}'], \n",
    "             label=f'⟨Z{j}⟩', alpha=0.7, linewidth=1.5)\n",
    "ax1.set_xlabel('Sample Index')\n",
    "ax1.set_ylabel('Expectation Value')\n",
    "ax1.set_title('PauliZ Expectation Values Across Parameter Samples')\n",
    "ax1.legend(loc='upper right')\n",
    "ax1.set_ylim(-1.1, 1.1)\n",
    "ax1.axhline(0, color='gray', linestyle='--', alpha=0.5)\n",
    "\n",
    "# 2. Distribution of expectation values\n",
    "ax2 = axes[0, 1]\n",
    "z_columns = [f'Z{j}' for j in range(n_wires)]\n",
    "colors = plt.cm.tab10(range(n_wires))\n",
    "for j, col in enumerate(z_columns):\n",
    "    ax2.hist(results_df[col], bins=15, alpha=0.5, color=colors[j], \n",
    "             label=f'⟨{col}⟩', edgecolor='black', linewidth=0.5)\n",
    "ax2.set_xlabel('Expectation Value')\n",
    "ax2.set_ylabel('Frequency')\n",
    "ax2.set_title('Distribution of Expectation Values')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Correlation heatmap between qubits\n",
    "ax3 = axes[1, 0]\n",
    "corr_matrix = results_df[z_columns].corr()\n",
    "sns.heatmap(corr_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            ax=ax3, vmin=-1, vmax=1, square=True,\n",
    "            xticklabels=[f'⟨Z{j}⟩' for j in range(n_wires)],\n",
    "            yticklabels=[f'⟨Z{j}⟩' for j in range(n_wires)])\n",
    "ax3.set_title('Correlation Between Qubit Expectation Values')\n",
    "\n",
    "# 4. Box plot of expectation values\n",
    "ax4 = axes[1, 1]\n",
    "results_df[z_columns].boxplot(ax=ax4)\n",
    "ax4.set_xlabel('Qubit')\n",
    "ax4.set_ylabel('Expectation Value')\n",
    "ax4.set_title('Expectation Value Statistics by Qubit')\n",
    "ax4.set_xticklabels([f'⟨Z{j}⟩' for j in range(n_wires)])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('quantum_circuit_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nStatistics for each qubit's expectation value:\")\n",
    "print(results_df[z_columns].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the quantum circuit\n",
    "print(\"Quantum Circuit Structure:\")\n",
    "print(qml.draw(quantum_circuit)(weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Throughput Benchmarking <a id=\"throughput\"></a>\n",
    "\n",
    "This example demonstrates throughput benchmarking across different task counts and visualizes the scaling behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulated benchmark data (similar to pilot_ray_slurm_throughput.py output)\n",
    "# In production, this data would come from running actual benchmarks\n",
    "\n",
    "benchmark_data = {\n",
    "    'cores': [64, 64, 64, 64, 128, 128, 128, 128, 256, 256, 256, 256],\n",
    "    'tasks': [128, 256, 512, 1024, 128, 256, 512, 1024, 128, 256, 512, 1024],\n",
    "    'runtime_secs': [2.1, 3.8, 7.2, 14.1, 1.2, 2.1, 3.9, 7.5, 0.8, 1.3, 2.2, 4.1],\n",
    "}\n",
    "\n",
    "benchmark_df = pd.DataFrame(benchmark_data)\n",
    "benchmark_df['throughput'] = benchmark_df['tasks'] / benchmark_df['runtime_secs']\n",
    "benchmark_df['efficiency'] = benchmark_df['throughput'] / benchmark_df['cores']\n",
    "\n",
    "benchmark_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize throughput benchmarks\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Throughput vs Task Count (grouped by cores)\n",
    "ax1 = axes[0, 0]\n",
    "core_configs = benchmark_df['cores'].unique()\n",
    "width = 0.25\n",
    "x = np.arange(len(benchmark_df['tasks'].unique()))\n",
    "task_counts = sorted(benchmark_df['tasks'].unique())\n",
    "\n",
    "for i, cores in enumerate(sorted(core_configs)):\n",
    "    data = benchmark_df[benchmark_df['cores'] == cores]\n",
    "    throughputs = [data[data['tasks'] == t]['throughput'].values[0] for t in task_counts]\n",
    "    ax1.bar(x + i * width, throughputs, width, label=f'{cores} cores')\n",
    "\n",
    "ax1.set_xlabel('Number of Tasks')\n",
    "ax1.set_ylabel('Throughput (tasks/second)')\n",
    "ax1.set_title('Throughput vs Task Count by Core Configuration')\n",
    "ax1.set_xticks(x + width)\n",
    "ax1.set_xticklabels(task_counts)\n",
    "ax1.legend(title='Configuration')\n",
    "\n",
    "# 2. Scaling efficiency\n",
    "ax2 = axes[0, 1]\n",
    "for tasks in sorted(benchmark_df['tasks'].unique()):\n",
    "    data = benchmark_df[benchmark_df['tasks'] == tasks].sort_values('cores')\n",
    "    ax2.plot(data['cores'], data['throughput'], 'o-', \n",
    "             label=f'{tasks} tasks', linewidth=2, markersize=8)\n",
    "\n",
    "# Add ideal scaling line\n",
    "ideal_x = np.array([64, 128, 256])\n",
    "ideal_y = ideal_x * (benchmark_df[benchmark_df['cores'] == 64]['throughput'].mean() / 64)\n",
    "ax2.plot(ideal_x, ideal_y, 'k--', alpha=0.5, label='Ideal scaling')\n",
    "\n",
    "ax2.set_xlabel('Number of Cores')\n",
    "ax2.set_ylabel('Throughput (tasks/second)')\n",
    "ax2.set_title('Throughput Scaling with Core Count')\n",
    "ax2.legend()\n",
    "ax2.set_xscale('log', base=2)\n",
    "ax2.set_yscale('log', base=2)\n",
    "\n",
    "# 3. Runtime comparison\n",
    "ax3 = axes[1, 0]\n",
    "pivot_runtime = benchmark_df.pivot(index='tasks', columns='cores', values='runtime_secs')\n",
    "pivot_runtime.plot(kind='bar', ax=ax3, width=0.8)\n",
    "ax3.set_xlabel('Number of Tasks')\n",
    "ax3.set_ylabel('Runtime (seconds)')\n",
    "ax3.set_title('Runtime Comparison Across Configurations')\n",
    "ax3.legend(title='Cores', loc='upper left')\n",
    "ax3.tick_params(axis='x', rotation=0)\n",
    "\n",
    "# 4. Efficiency heatmap\n",
    "ax4 = axes[1, 1]\n",
    "pivot_efficiency = benchmark_df.pivot(index='tasks', columns='cores', values='efficiency')\n",
    "sns.heatmap(pivot_efficiency, annot=True, fmt='.2f', cmap='YlGnBu', ax=ax4)\n",
    "ax4.set_xlabel('Number of Cores')\n",
    "ax4.set_ylabel('Number of Tasks')\n",
    "ax4.set_title('Parallel Efficiency (tasks/sec/core)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('throughput_benchmark.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nBenchmark Summary:\")\n",
    "print(f\"Max throughput: {benchmark_df['throughput'].max():.1f} tasks/second\")\n",
    "print(f\"Best configuration: {benchmark_df.loc[benchmark_df['throughput'].idxmax(), 'cores']} cores\")\n",
    "print(f\"Avg efficiency: {benchmark_df['efficiency'].mean():.3f} tasks/sec/core\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Metrics Analysis <a id=\"metrics\"></a>\n",
    "\n",
    "This example demonstrates how to analyze the metrics.csv file generated by QTAU during task execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample metrics data (simulating metrics.csv output)\n",
    "np.random.seed(42)\n",
    "n_metrics = 100\n",
    "\n",
    "base_time = pd.Timestamp('2024-01-15 10:00:00')\n",
    "submit_times = [base_time + pd.Timedelta(seconds=i * 0.1 + np.random.uniform(0, 0.05)) \n",
    "                for i in range(n_metrics)]\n",
    "\n",
    "metrics_data = {\n",
    "    'task_id': [f'task-{uuid.uuid4().hex[:8]}' for _ in range(n_metrics)] if 'uuid' in dir() else [f'task-{i:08d}' for i in range(n_metrics)],\n",
    "    'pilot_scheduled': np.random.choice(['pilot-1', 'pilot-2', 'pilot-3'], n_metrics),\n",
    "    'submit_time': submit_times,\n",
    "    'wait_time_secs': np.random.exponential(0.2, n_metrics),\n",
    "    'staging_time_secs': np.random.exponential(0.05, n_metrics),\n",
    "    'input_staging_data_size_bytes': np.random.randint(1000, 100000, n_metrics),\n",
    "    'execution_secs': np.random.exponential(0.5, n_metrics),\n",
    "    'status': np.random.choice(['SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'FAILED'], n_metrics),\n",
    "    'error_msg': [None if s == 'SUCCESS' else 'Timeout' for s in np.random.choice(['SUCCESS', 'SUCCESS', 'SUCCESS', 'SUCCESS', 'FAILED'], n_metrics)]\n",
    "}\n",
    "\n",
    "# Fix error_msg to match status\n",
    "for i in range(n_metrics):\n",
    "    if metrics_data['status'][i] == 'SUCCESS':\n",
    "        metrics_data['error_msg'][i] = None\n",
    "\n",
    "metrics_df = pd.DataFrame(metrics_data)\n",
    "metrics_df['completion_time'] = metrics_df['submit_time'] + pd.to_timedelta(\n",
    "    metrics_df['wait_time_secs'] + metrics_df['execution_secs'], unit='s')\n",
    "metrics_df['total_time_secs'] = metrics_df['wait_time_secs'] + metrics_df['execution_secs']\n",
    "\n",
    "metrics_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize metrics analysis\n",
    "fig, axes = plt.subplots(2, 3, figsize=(16, 10))\n",
    "\n",
    "# 1. Task status pie chart\n",
    "ax1 = axes[0, 0]\n",
    "status_counts = metrics_df['status'].value_counts()\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "ax1.pie(status_counts.values, labels=status_counts.index, autopct='%1.1f%%',\n",
    "        colors=colors[:len(status_counts)], explode=[0.05] * len(status_counts))\n",
    "ax1.set_title('Task Success Rate')\n",
    "\n",
    "# 2. Tasks per pilot\n",
    "ax2 = axes[0, 1]\n",
    "pilot_counts = metrics_df['pilot_scheduled'].value_counts()\n",
    "ax2.bar(pilot_counts.index, pilot_counts.values, color=plt.cm.Set2.colors[:len(pilot_counts)])\n",
    "ax2.set_xlabel('Pilot')\n",
    "ax2.set_ylabel('Number of Tasks')\n",
    "ax2.set_title('Task Distribution Across Pilots')\n",
    "for i, v in enumerate(pilot_counts.values):\n",
    "    ax2.text(i, v + 1, str(v), ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "# 3. Execution time by pilot\n",
    "ax3 = axes[0, 2]\n",
    "metrics_df.boxplot(column='execution_secs', by='pilot_scheduled', ax=ax3)\n",
    "ax3.set_xlabel('Pilot')\n",
    "ax3.set_ylabel('Execution Time (seconds)')\n",
    "ax3.set_title('Execution Time Distribution by Pilot')\n",
    "plt.suptitle('')  # Remove automatic title\n",
    "\n",
    "# 4. Time breakdown\n",
    "ax4 = axes[1, 0]\n",
    "time_components = ['wait_time_secs', 'staging_time_secs', 'execution_secs']\n",
    "time_means = metrics_df[time_components].mean()\n",
    "ax4.bar(range(len(time_components)), time_means.values, \n",
    "        color=['#3498db', '#9b59b6', '#e74c3c'])\n",
    "ax4.set_xticks(range(len(time_components)))\n",
    "ax4.set_xticklabels(['Wait', 'Staging', 'Execution'])\n",
    "ax4.set_ylabel('Average Time (seconds)')\n",
    "ax4.set_title('Average Time Breakdown')\n",
    "for i, v in enumerate(time_means.values):\n",
    "    ax4.text(i, v + 0.01, f'{v:.3f}s', ha='center', va='bottom')\n",
    "\n",
    "# 5. Data staging size distribution\n",
    "ax5 = axes[1, 1]\n",
    "ax5.hist(metrics_df['input_staging_data_size_bytes'] / 1024, bins=20, \n",
    "         color='steelblue', edgecolor='black', alpha=0.7)\n",
    "ax5.set_xlabel('Input Data Size (KB)')\n",
    "ax5.set_ylabel('Frequency')\n",
    "ax5.set_title('Input Data Size Distribution')\n",
    "ax5.axvline(metrics_df['input_staging_data_size_bytes'].mean() / 1024, \n",
    "            color='red', linestyle='--', label=f'Mean: {metrics_df[\"input_staging_data_size_bytes\"].mean()/1024:.1f} KB')\n",
    "ax5.legend()\n",
    "\n",
    "# 6. Execution time vs Data size\n",
    "ax6 = axes[1, 2]\n",
    "successful_tasks = metrics_df[metrics_df['status'] == 'SUCCESS']\n",
    "ax6.scatter(successful_tasks['input_staging_data_size_bytes'] / 1024, \n",
    "            successful_tasks['execution_secs'], alpha=0.5, c='steelblue')\n",
    "ax6.set_xlabel('Input Data Size (KB)')\n",
    "ax6.set_ylabel('Execution Time (seconds)')\n",
    "ax6.set_title('Execution Time vs Input Data Size')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('metrics_analysis.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nMetrics Summary:\")\n",
    "print(f\"Total tasks: {len(metrics_df)}\")\n",
    "print(f\"Success rate: {(metrics_df['status'] == 'SUCCESS').mean() * 100:.1f}%\")\n",
    "print(f\"Avg execution time: {metrics_df['execution_secs'].mean():.3f}s\")\n",
    "print(f\"Avg wait time: {metrics_df['wait_time_secs'].mean():.3f}s\")\n",
    "print(f\"Total runtime: {(metrics_df['completion_time'].max() - metrics_df['submit_time'].min()).total_seconds():.2f}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Hybrid Quantum-Classical Optimization <a id=\"hybrid\"></a>\n",
    "\n",
    "This example demonstrates a hybrid quantum-classical optimization workflow, similar to the PennyLane hybrid computation example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define quantum and classical devices\n",
    "dev_qubit = qml.device(\"default.qubit\", wires=1)\n",
    "\n",
    "@qml.qnode(dev_qubit)\n",
    "def qubit_rotation(phi1, phi2):\n",
    "    \"\"\"Qubit rotation circuit.\"\"\"\n",
    "    qml.RX(phi1, wires=0)\n",
    "    qml.RY(phi2, wires=0)\n",
    "    return qml.expval(qml.PauliZ(0))\n",
    "\n",
    "def target_function(phi1, phi2):\n",
    "    \"\"\"Target function to match.\"\"\"\n",
    "    return np.cos(phi1) * np.cos(phi2)\n",
    "\n",
    "def cost_function(params):\n",
    "    \"\"\"Cost function: squared difference between quantum and target.\"\"\"\n",
    "    phi1, phi2 = 0.5, 0.8  # Fixed target parameters\n",
    "    qubit_result = qubit_rotation(params[0], params[1])\n",
    "    target_result = target_function(phi1, phi2)\n",
    "    return (qubit_result - target_result) ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimization\n",
    "opt = qml.GradientDescentOptimizer(stepsize=0.4)\n",
    "params = pnp.array([0.01, 0.01], requires_grad=True)\n",
    "n_steps = 100\n",
    "\n",
    "# Track optimization progress\n",
    "cost_history = []\n",
    "param_history = [params.copy()]\n",
    "\n",
    "for i in range(n_steps):\n",
    "    params = opt.step(cost_function, params)\n",
    "    cost_history.append(float(cost_function(params)))\n",
    "    param_history.append(params.copy())\n",
    "\n",
    "param_history = np.array(param_history)\n",
    "print(f\"Initial parameters: [{param_history[0][0]:.4f}, {param_history[0][1]:.4f}]\")\n",
    "print(f\"Final parameters: [{params[0]:.4f}, {params[1]:.4f}]\")\n",
    "print(f\"Final cost: {cost_history[-1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize optimization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# 1. Cost vs training step\n",
    "ax1 = axes[0, 0]\n",
    "ax1.plot(range(n_steps), cost_history, 'b-', linewidth=2)\n",
    "ax1.set_xlabel('Training Step')\n",
    "ax1.set_ylabel('Cost')\n",
    "ax1.set_title('Optimization Convergence')\n",
    "ax1.set_yscale('log')\n",
    "ax1.axhline(0.001, color='red', linestyle='--', alpha=0.5, label='Convergence threshold')\n",
    "ax1.legend()\n",
    "\n",
    "# 2. Parameter evolution\n",
    "ax2 = axes[0, 1]\n",
    "ax2.plot(range(n_steps + 1), param_history[:, 0], 'r-', label='θ₁', linewidth=2)\n",
    "ax2.plot(range(n_steps + 1), param_history[:, 1], 'g-', label='θ₂', linewidth=2)\n",
    "ax2.set_xlabel('Training Step')\n",
    "ax2.set_ylabel('Parameter Value')\n",
    "ax2.set_title('Parameter Evolution During Training')\n",
    "ax2.legend()\n",
    "\n",
    "# 3. Cost landscape with optimization path\n",
    "ax3 = axes[1, 0]\n",
    "theta1_range = np.linspace(-np.pi, np.pi, 100)\n",
    "theta2_range = np.linspace(-np.pi, np.pi, 100)\n",
    "T1, T2 = np.meshgrid(theta1_range, theta2_range)\n",
    "Z = np.zeros_like(T1)\n",
    "\n",
    "for i in range(len(theta1_range)):\n",
    "    for j in range(len(theta2_range)):\n",
    "        Z[j, i] = cost_function(pnp.array([theta1_range[i], theta2_range[j]]))\n",
    "\n",
    "contour = ax3.contourf(T1, T2, Z, levels=50, cmap='viridis')\n",
    "plt.colorbar(contour, ax=ax3, label='Cost')\n",
    "ax3.plot(param_history[:, 0], param_history[:, 1], 'r.-', markersize=3, linewidth=1, alpha=0.7)\n",
    "ax3.plot(param_history[0, 0], param_history[0, 1], 'go', markersize=10, label='Start')\n",
    "ax3.plot(param_history[-1, 0], param_history[-1, 1], 'r*', markersize=15, label='End')\n",
    "ax3.set_xlabel('θ₁')\n",
    "ax3.set_ylabel('θ₂')\n",
    "ax3.set_title('Cost Landscape with Optimization Path')\n",
    "ax3.legend()\n",
    "\n",
    "# 4. Comparison: Quantum vs Target\n",
    "ax4 = axes[1, 1]\n",
    "phi_range = np.linspace(0, 2*np.pi, 50)\n",
    "quantum_results = []\n",
    "target_results = []\n",
    "\n",
    "for phi in phi_range:\n",
    "    quantum_results.append(float(qubit_rotation(params[0], phi)))\n",
    "    target_results.append(target_function(0.5, phi))\n",
    "\n",
    "ax4.plot(phi_range, quantum_results, 'b-', label='Quantum Circuit', linewidth=2)\n",
    "ax4.plot(phi_range, target_results, 'r--', label='Target Function', linewidth=2)\n",
    "ax4.set_xlabel('φ₂')\n",
    "ax4.set_ylabel('Output Value')\n",
    "ax4.set_title('Quantum Circuit vs Target Function')\n",
    "ax4.legend()\n",
    "ax4.set_xlim(0, 2*np.pi)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('hybrid_optimization.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nOptimization Summary:\")\n",
    "print(f\"Initial cost: {cost_function(pnp.array([0.01, 0.01])):.6f}\")\n",
    "print(f\"Final cost: {cost_history[-1]:.6f}\")\n",
    "print(f\"Cost reduction: {(1 - cost_history[-1]/cost_function(pnp.array([0.01, 0.01]))) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated various QTAU use cases with visualizations:\n",
    "\n",
    "1. **Basic Task Distribution**: Visualized task execution timelines, wait/execution time distributions\n",
    "2. **PennyLane Quantum Circuits**: Showed expectation value distributions and correlations\n",
    "3. **Throughput Benchmarking**: Analyzed scaling behavior and parallel efficiency\n",
    "4. **Metrics Analysis**: Demonstrated how to analyze QTAU-generated metrics\n",
    "5. **Hybrid Optimization**: Visualized quantum-classical optimization convergence\n",
    "\n",
    "### Running with Actual QTAU\n",
    "\n",
    "To run these examples with actual distributed execution:\n",
    "\n",
    "```python\n",
    "# Initialize PilotComputeService\n",
    "pcs = PilotComputeService(\n",
    "    execution_engine=ExecutionEngine.RAY,  # or ExecutionEngine.DASK\n",
    "    working_directory=WORKING_DIRECTORY\n",
    ")\n",
    "\n",
    "# Create a pilot\n",
    "pilot = pcs.create_pilot(pilot_compute_description)\n",
    "pilot.wait()\n",
    "\n",
    "# Submit tasks\n",
    "tasks = [pcs.submit_task(my_function, arg) for arg in args]\n",
    "\n",
    "# Wait and get results\n",
    "pcs.wait_tasks(tasks)\n",
    "results = pcs.get_results(tasks)\n",
    "\n",
    "# Read metrics from CSV\n",
    "metrics_df = pd.read_csv(os.path.join(pcs.pcs_working_directory, 'metrics.csv'))\n",
    "\n",
    "# Clean up\n",
    "pcs.cancel()\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all figures for reference\n",
    "print(\"Generated visualization files:\")\n",
    "print(\"- task_execution_visualization.png\")\n",
    "print(\"- quantum_circuit_results.png\")\n",
    "print(\"- throughput_benchmark.png\")\n",
    "print(\"- metrics_analysis.png\")\n",
    "print(\"- hybrid_optimization.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
